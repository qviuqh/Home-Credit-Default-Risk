{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T14:27:37.416810Z",
     "start_time": "2020-10-30T14:27:17.121329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing Useful DataStructures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "\n",
    "#importing plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "#importing Misc Libraries\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "\n",
    "#for 100% jupyter notebook cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import XGBClassifier từ thư viện XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Import LGBMClassifier từ thư viện LightGBM\n",
    "from lightgbm import LGBMClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import XGBRegressor từ thư viện XGBoost\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LGBMRegressor từ thư viện LightGBM\n",
    "from lightgbm import LGBMRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Utility Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T06:36:52.363937Z",
     "start_time": "2020-10-26T06:36:52.350972Z"
    }
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(data, verbose = True):\n",
    "    '''\n",
    "    This function is used to reduce the memory usage by converting the datatypes of a pandas\n",
    "    DataFrame withing required limits.\n",
    "    '''\n",
    "    \n",
    "    start_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('-'*100)\n",
    "        print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "\n",
    "    end_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "        print('-'*100)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T08:39:34.421193Z",
     "start_time": "2020-10-26T08:39:34.402244Z"
    }
   },
   "outputs": [],
   "source": [
    "def relational_tables_prepare(file_directory = '', verbose = True):\n",
    "    '''\n",
    "    Function to pickle the relational tables which would need to be merged during production with the \n",
    "    test datapoint\n",
    "    \n",
    "    Inputs:\n",
    "        file_directory: str, default = ''\n",
    "            The directory in which files are saved\n",
    "        verbose: bool, default = True\n",
    "            Whether to keep verbosity or not\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Loading the tables into memory...\")\n",
    "        start = datetime.now()\n",
    "        \n",
    "    #loading all the tables in memory, for dimensionality reduction\n",
    "    with open(file_directory + 'bureau_merged_preprocessed.pkl', 'rb') as f:\n",
    "        bureau_aggregated = reduce_mem_usage(pickle.load(f), verbose = False)\n",
    "    with open(file_directory + 'previous_application_preprocessed.pkl', 'rb') as f:\n",
    "        previous_aggregated = reduce_mem_usage(pickle.load(f), verbose = False)\n",
    "    with open(file_directory + 'installments_payments_preprocessed.pkl', 'rb') as f:\n",
    "        installments_aggregated = reduce_mem_usage(pickle.load(f), verbose = False)\n",
    "    with open(file_directory + 'POS_CASH_balance_preprocessed.pkl', 'rb') as f:\n",
    "        pos_aggregated = reduce_mem_usage(pickle.load(f), verbose = False)\n",
    "    with open(file_directory + 'credit_card_balance_preprocessed.pkl', 'rb') as f:\n",
    "        cc_aggregated = reduce_mem_usage(pickle.load(f), verbose = False)\n",
    "    with open(file_directory + 'application_train_preprocessed.pkl', 'rb') as f:\n",
    "        application_train = reduce_mem_usage(pickle.load(f), verbose = False) \n",
    "    with open(file_directory + 'application_test_preprocessed.pkl', 'rb') as f:\n",
    "        application_test = reduce_mem_usage(pickle.load(f), verbose = False)\n",
    "    with open('Final_XGBOOST_Selected_features.pkl', 'rb') as f:\n",
    "        final_cols = pickle.load(f)\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"Done.\")\n",
    "        print(f\"Time Elapsed = {datetime.now() - start}\")\n",
    "        start2 = datetime.now()\n",
    "        print(\"\\nRemoving the non-useful features...\")\n",
    "    #removing non-useful columns from pre-processed previous_application table\n",
    "    previous_app_columns_to_keep = set(previous_aggregated.columns).intersection(set(final_cols)).union(\n",
    "                                    set([ele for ele in previous_aggregated.columns if 'AMT_ANNUITY' in ele] + [ele for ele in previous_aggregated.columns if 'AMT_GOODS' in ele]))\n",
    "    previous_aggregated = previous_aggregated[previous_app_columns_to_keep]\n",
    "    #removing non-useful columns from pre-processed credit_card_balance table\n",
    "    credit_card_balance_columns_to_keep = set(cc_aggregated.columns).intersection(set(final_cols)).union(\n",
    "                                    set([ele for ele in cc_aggregated.columns if 'AMT_RECEIVABLE_PRINCIPAL' in ele] + \n",
    "                                        [ele for ele in cc_aggregated.columns if 'AMT_RECIVABLE' in ele] + \n",
    "                                        [ele for ele in cc_aggregated.columns if 'TOTAL_RECEIVABLE' in ele] + ['SK_ID_CURR']))\n",
    "    cc_aggregated = cc_aggregated[credit_card_balance_columns_to_keep]\n",
    "    #removing non-useful columns from pre-processed installments_payments table\n",
    "    installments_payments_columns_to_keep = set(installments_aggregated.columns).intersection(set(final_cols)).union(\n",
    "                                            set([ele for ele in installments_aggregated.columns if 'AMT_PAYMENT' in \n",
    "                                                 ele and 'RATIO' not in ele and 'DIFF' not in ele] + ['AMT_INSTALMENT_MEAN_MAX', 'AMT_INSTALMENT_SUM_MAX']))\n",
    "    installments_aggregated = installments_aggregated[installments_payments_columns_to_keep]\n",
    "    #removing non-useful columns from pre-processed bureau-aggregated table\n",
    "    bureau_columns_to_keep =  set(bureau_aggregated.columns).intersection(set(final_cols)).union([ele for ele in bureau_aggregated.columns\n",
    "                                        if 'DAYS_CREDIT' in ele and 'ENDDATE' not in ele and 'UPDATE' not in ele] + [ele for ele in bureau_aggregated.columns if\n",
    "                                        'AMT_CREDIT' in ele and 'OVERDUE' in ele] + [ele for ele in bureau_aggregated.columns if 'AMT_ANNUITY' in ele and 'CREDIT'  not in ele])\n",
    "    bureau_aggregated = bureau_aggregated[bureau_columns_to_keep]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Done.\")\n",
    "        print(f\"Time Elapsed = {datetime.now() - start2}\")\n",
    "        print(\"\\nMerging all the tables, and saving to pickle file 'relational_table.pkl'...\")\n",
    "\n",
    "    #merging all the tables\n",
    "    relational_table = cc_aggregated.merge(bureau_aggregated, on = 'SK_ID_CURR', how = 'outer')\n",
    "    relational_table = relational_table.merge(previous_aggregated, on = 'SK_ID_CURR', how = 'outer')\n",
    "    relational_table = relational_table.merge(installments_aggregated, on = 'SK_ID_CURR', how = 'outer')\n",
    "    relational_table = relational_table.merge(pos_aggregated, on = 'SK_ID_CURR', how = 'outer')\n",
    "    relational_table = reduce_mem_usage(relational_table, verbose = False)\n",
    "\n",
    "    with open(file_directory + 'relational_table.pkl', 'wb') as f:\n",
    "        pickle.dump(relational_table, f)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Done.\")\n",
    "        print(f\"Total Time taken = {datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T06:37:06.138811Z",
     "start_time": "2020-10-26T06:37:06.104902Z"
    }
   },
   "outputs": [],
   "source": [
    "class Boosting:\n",
    "    '''\n",
    "    Class for Boosting Ensembles and displaying results. Contains 6 methods:\n",
    "    \n",
    "        1. init method\n",
    "        2. train method\n",
    "        3. proba_to_class method\n",
    "        4. tune_threshold method\n",
    "        5. results method\n",
    "        6. feat_importance_show\n",
    "    '''\n",
    "\n",
    "    def __init__(self, x_train, y_train, x_test, params, num_folds = 3, random_state = 33, verbose = True, save_model_to_pickle = False):\n",
    "        '''\n",
    "        Function to initialize the class members.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            x_train: DataFrame\n",
    "                Training DataFrame\n",
    "            y_train: DataFrame\n",
    "                Training Class labels\n",
    "            x_test: DataFrame\n",
    "                Test DataFrame\n",
    "            params: dict\n",
    "                Parameters for the boosting ensemble\n",
    "            num_folds: int, default = 3\n",
    "                Number of folds for k-Fold Cross Validation\n",
    "            random_state: int, default = 33\n",
    "                Random State for Splitting the data for K-Fold Cross Validation\n",
    "            verbose: bool, default = True\n",
    "                Whether to keep verbosity or not\n",
    "            save_model_to_pickle: bool, default = False\n",
    "                Whether to save the model to pickle file or not\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.params = params\n",
    "        self.num_folds = num_folds\n",
    "        self.stratified_cv = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = random_state)\n",
    "        self.verbose = verbose\n",
    "        self.save_model = save_model_to_pickle\n",
    "        \n",
    "    def train(self, booster, verbose = 400, early_stopping = 200, pickle_name = ''):\n",
    "        '''\n",
    "        Function to train the Classifier on given parameters. It fits the classifier for each fold, and for Cross Validation,\n",
    "        uses Out-of-Fold Predictions. The test predictions are averaged predictions over each fold.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            booster: str\n",
    "                Whether the booster is 'xgboost' or 'lightgbm'\n",
    "            verbose: int, default = 400\n",
    "                Number of boosting rounds for printint boosting results.\n",
    "            early_stopping: int, default = 200\n",
    "                Number of boosting rounds to look for early stopping\n",
    "            pickle_name: str, default = ''\n",
    "                The string to add to end of pickle file of model, if any\n",
    "        \n",
    "        Returns:\n",
    "            None        \n",
    "        '''\n",
    "        \n",
    "        self.train_preds_proba_mean = np.zeros(self.x_train.shape[0])\n",
    "        #out-of-fold cv predictions\n",
    "        self.cv_preds_proba = np.zeros(self.x_train.shape[0])\n",
    "        self.test_preds_proba_mean = np.zeros(self.x_test.shape[0])\n",
    "        #best threshold will be \n",
    "        self.best_threshold_train = 0\n",
    "        self.feature_importance = pd.DataFrame()\n",
    "        self.feature_importance['features'] = self.x_train.columns\n",
    "        self.feature_importance['gain'] = np.zeros(self.x_train.shape[1])\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"Fitting the {booster} on Training Data with {self.num_folds} fold cross validation, and using Out-Of-Folds Predictions for Cross-Validation\")\n",
    "            start = datetime.now()\n",
    "        \n",
    "        for fold_number, (train_indices, cv_indices) in enumerate(self.stratified_cv.split(self.x_train, self.y_train), 1):\n",
    "            if self.verbose:\n",
    "                print(f\"\\n\\tFold Number {fold_number}\\n\")\n",
    "            \n",
    "            x_tr = self.x_train.iloc[train_indices]\n",
    "            y_tr = self.y_train.iloc[train_indices]\n",
    "            x_cv = self.x_train.iloc[cv_indices]\n",
    "            y_cv = self.y_train.iloc[cv_indices]\n",
    "            \n",
    "            if booster == 'xgboost':\n",
    "                clf = XGBClassifier(**self.params)\n",
    "            else:\n",
    "                clf = LGBMClassifier(**self.params)\n",
    "                \n",
    "            clf.fit(x_tr, y_tr, eval_set = [(x_tr, y_tr), (x_cv, y_cv)], eval_metric = 'auc',\n",
    "                     verbose = verbose, early_stopping_rounds = 200)\n",
    "            \n",
    "            if booster == 'xgboost':\n",
    "                self.train_preds_proba_mean[train_indices] = clf.predict_proba(x_tr, ntree_limit = clf.get_booster().best_ntree_limit)[:, 1] / (self.num_folds - 1)\n",
    "                self.cv_preds_proba[cv_indices] = clf.predict_proba(x_cv, ntree_limit = clf.get_booster().best_ntree_limit)[:,1]\n",
    "                self.test_preds_proba_mean += clf.predict_proba(self.x_test, ntree_limit = clf.get_booster().best_ntree_limit)[:,1] / self.num_folds\n",
    "\n",
    "                #feature importance\n",
    "                gain_fold = clf.get_booster().get_score(importance_type = 'gain')\n",
    "                feat_imp = pd.DataFrame()\n",
    "                feat_imp['features'] = gain_fold.keys()\n",
    "                feat_imp['gain'] = gain_fold.values()\n",
    "            \n",
    "            else:\n",
    "                self.train_preds_proba_mean[train_indices] = clf.predict_proba(x_tr, num_iteration = clf.best_iteration_)[:,1] / (self.num_folds - 1)\n",
    "                self.cv_preds_proba[cv_indices] = clf.predict_proba(x_cv, num_iteration = clf.best_iteration_)[:,1]\n",
    "                self.test_preds_proba_mean += clf.predict_proba(self.x_test, num_iteration = clf.best_iteration_)[:,1] / self.num_folds\n",
    "\n",
    "                #feature importance\n",
    "                gain_fold = clf.booster_.feature_importance(importance_type='gain')\n",
    "                feat_imp = pd.DataFrame()\n",
    "                feat_imp['features'] = self.x_train.columns\n",
    "                feat_imp['gain'] = gain_fold\n",
    "            \n",
    "            #tuning the threshold for optimal TPR and FPR from ROC Curve\n",
    "            self.best_threshold_train += self.tune_threshold(self.y_train[train_indices], self.train_preds_proba_mean[train_indices]) / self.num_folds\n",
    "            #concatenating the feature importance of each fold to original df\n",
    "            self.feature_importance = pd.concat([self.feature_importance, feat_imp], axis = 0)\n",
    "\n",
    "            if self.save_model:\n",
    "                #saving the model into a pickle file\n",
    "                with open(f'clf_{booster}_fold_{fold_number}_model_{pickle_name}.pkl', 'wb') as f:\n",
    "                    pickle.dump(clf, f)\n",
    "          \n",
    "        #mean feature importance averaged over all folds\n",
    "        self.feature_importance = self.feature_importance.groupby('features', as_index = False).mean()\n",
    "        #sorting the feature importance\n",
    "        self.feature_importance = self.feature_importance.sort_values(by = 'gain', ascending = False)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Done.\")\n",
    "            print(f\"Time elapsed = {datetime.now() - start}\")\n",
    "        gc.collect()\n",
    "        \n",
    "    def proba_to_class(self, proba, threshold):\n",
    "        '''\n",
    "        Function to convert a given probability to class label based on a threshold value.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            proba: numpy array\n",
    "                Probabilities of class label = 1\n",
    "            threshold: int\n",
    "                Threshold probability to be considered as Positive or Negative Class Label\n",
    "            \n",
    "        Returns:\n",
    "            Converted Class Label\n",
    "        '''\n",
    "\n",
    "        return np.where(proba >= threshold, 1, 0)\n",
    "        \n",
    "    def tune_threshold(self, true_labels, predicted_probas):\n",
    "        '''\n",
    "        Function to find the optimal threshold for maximizing the TPR and minimizing the FPR from ROC-AUC Curve.\n",
    "        This is found out by using the J Statistic, which is J = TPR - FPR.\n",
    "        Reference: https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            true_labels: numpy array or pandas series\n",
    "                True Class Labels\n",
    "            predicted_probas: numpy array\n",
    "                Predicted Probability of Positive Class label\n",
    "            \n",
    "        Returns:\n",
    "            Threshold probability.\n",
    "        '''\n",
    "\n",
    "        fpr, tpr, threshold = roc_curve(true_labels, predicted_probas)\n",
    "        j_stat = tpr - fpr\n",
    "        index_for_best_threshold = np.argmax(j_stat)\n",
    "        \n",
    "        return threshold[index_for_best_threshold]\n",
    "           \n",
    "    def results(self, roc_auc = True, precision_recall = True, confusion_matrix = True, cv_test_distribution = False):\n",
    "        '''\n",
    "        Function to display the final results of Train, CV and Test Dataset.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        #getting the crisp class labels\n",
    "        self.train_preds_class = self.proba_to_class(self.train_preds_proba_mean, self.best_threshold_train)\n",
    "        self.cv_preds_class = self.proba_to_class(self.cv_preds_proba, self.best_threshold_train)\n",
    "        self.test_preds_class = self.proba_to_class(self.test_preds_proba_mean, self.best_threshold_train)\n",
    "        print(\"=\" * 100)\n",
    "        print(\"Train Results:\")\n",
    "        print(f\"\\nThe best selected Threshold as per the J-Statistic, which is J = TPR - FPR, is = {self.best_threshold_train}\\n\")\n",
    "        if roc_auc:\n",
    "            print(f\"\\tROC-AUC Score = {roc_auc_score(self.y_train, self.train_preds_proba_mean)}\")\n",
    "        if precision_recall:\n",
    "            print(f\"\\tPrecision Score = {precision_score(self.y_train, self.train_preds_class)}\")\n",
    "            print(f\"\\tRecall Score = {recall_score(self.y_train, self.train_preds_class)}\")\n",
    "        print(\"CV Results:\")\n",
    "        if roc_auc:\n",
    "            print(f\"\\tROC-AUC Score = {roc_auc_score(self.y_train, self.cv_preds_proba)}\")\n",
    "        if precision_recall:\n",
    "            print(f\"\\tPrecision Score = {precision_score(self.y_train, self.cv_preds_class)}\")\n",
    "            print(f\"\\tRecall Score = {recall_score(self.y_train, self.cv_preds_class)}\")\n",
    "\n",
    "        if confusion_matrix:\n",
    "            print('=' * 100)\n",
    "            print(\"Confusion, Precision and Recall Matrix on CV data:\")\n",
    "            conf_mat = confusion_matrix(self.y_train, self.cv_preds_class)\n",
    "            conf_mat = pd.DataFrame(conf_mat, columns = ['Predicted_0','Predicted_1'], index = ['Actual_0','Actual_1'])\n",
    "            plt.figure(figsize = (7,6))\n",
    "            plt.title('Confusion Matrix Heatmap')\n",
    "            sns.heatmap(conf_mat, annot = True, fmt = 'g', linewidth = 0.5, annot_kws = {'size' : 15})\n",
    "            plt.show()\n",
    "        \n",
    "        if cv_test_distribution:\n",
    "            print('=' * 100)\n",
    "            print(\"Distribution of Original Class Labels and Predicted CV and Test Class Labels\")\n",
    "            plt.figure(figsize = (20,6))\n",
    "            plt.subplot(1,3,1)\n",
    "            plt.title('Class Distribution of Original Dataset')\n",
    "            sns.countplot(self.y_train)\n",
    "            plt.subplot(1,3,2)\n",
    "            plt.title('Class Distribution of predicted Class Labels on CV')\n",
    "            sns.countplot(self.cv_preds_class)\n",
    "            plt.subplot(1,3,3)\n",
    "            plt.title('Class Distribution of predicted Test Dataset')\n",
    "            sns.countplot(self.test_preds_class)\n",
    "            plt.show()\n",
    "        print('=' * 100)\n",
    "        \n",
    "        gc.collect()\n",
    "    \n",
    "    def feat_importances_show(self, num_features, figsize = (10,15)):\n",
    "        '''\n",
    "        Function to display the top most important features.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            num_features: int\n",
    "                Number of top features importances to display\n",
    "            figsize: tuple, default = (10,15)\n",
    "                Size of figure to be displayed\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        plt.figure(figsize = figsize) \n",
    "        sns.barplot(self.feature_importance['gain'].iloc[:num_features], self.feature_importance['features'].iloc[:num_features], orient = 'h')\n",
    "        plt.title(f'Top {num_features} features as per classifier')\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.ylabel('Feature Names')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        print('=' * 100)\n",
    "        \n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains several number of relational tables. We'll process each one of them separately, and then finally in the end, merge all of them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bureau_balance.csv and bureau.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tables contain the information related to the client's previous credits which were not with Home Credit Group, and were reported by Credit Bureau Department. \n",
    "<ol><li><b>bureau_balance</b>\n",
    "    <ol><li>First off, the bureau_balance table contains three fields, i.e. SK_ID_BUREAU, MONTHS_BALANCE and STATUS.</li>\n",
    "        <li>Since the Status follows somewhat ordinal behaviour, we start by label encoding it.</li>\n",
    "        <li>Next, some features are created such as weighted status, which is obtained by dividing the status by the MONTHS_BALANCE.</li>\n",
    "        <li>Since the data contains the timeseries, we also calculate the Exponential Weighted Moving Average of the Status and Weighted Status fields.</li>\n",
    "        <li>Finally, we aggregate the data over SK_ID_BUREAU, in such a way that we first aggregate it over all the data, and after that we also aggregate over the last 2 years. These 2 years would depict the more recent behaviour of the clients.</li>\n",
    "        <li>The aggregations performed are based on Domain Knowledge, such as mean, min, max, sum, count, etc. For EDA features, we only take the last/most recent values, as they somewhat contain the trend of all the previous values.</li></ol>\n",
    "    <li><b>bureau</b>\n",
    "    <ol><li>Firstly, we merge the bureau table with the aggregated bureau_balance table from previous step, on SK_ID_BUREAU.</li>\n",
    "        <li>We replace some erroneous values with NaN values. We saw some loans dating back to as long as 100 years ago. We believe they wouldn't really tell much about client's recent behaviour, so we remove them and only keep the loans in the period of 50 years.</li>\n",
    "        <li>We create some features by multiplications, divisions, subtractions of raw features, based on domain knowledge, such as Credit duration, annutiy to credit ratio, etc.</li>\n",
    "        <li>The categorical features are one-hot encoded.\n",
    "        <li>To merge these to main table, i.e. application_train, we aggregate this table over SK_ID_CURR. We perform the aggregations again in two ways. We aggregate the credits based on the CREDIT_ACTIVE category, where we aggregate for two most popular categories separately, i.e. Active, and Closed. Later we aggregate for the remaining categories too, and merge these. We aggregated the whole data overall too. The aggregations performed are sum, mean, min, max, last, etc.</li>\n",
    "        </ol></li></ol>\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T07:43:43.318079Z",
     "start_time": "2020-10-23T07:43:43.271205Z"
    }
   },
   "outputs": [],
   "source": [
    "class preprocess_bureau_balance_and_bureau:\n",
    "    '''\n",
    "    Preprocess the tables bureau_balance and bureau.\n",
    "    Contains 4 member functions:\n",
    "        1. init method\n",
    "        2. preprocess_bureau_balance method\n",
    "        3. preprocess_bureau method\n",
    "        4. main method\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, file_directory = '', verbose = True, dump_to_pickle = False):\n",
    "        '''\n",
    "        This function is used to initialize the class members \n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            file_directory: Path, str, default = ''\n",
    "                The path where the file exists. Include a '/' at the end of the path in input\n",
    "            verbose: bool, default = True\n",
    "                Whether to enable verbosity or not\n",
    "            dump_to_pickle: bool, default = False\n",
    "                Whether to pickle the final preprocessed table or not\n",
    "                \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        self.file_directory = file_directory\n",
    "        self.verbose = verbose\n",
    "        self.dump_to_pickle = dump_to_pickle\n",
    "        self.start = datetime.now()\n",
    "        \n",
    "    def preprocess_bureau_balance(self):\n",
    "        '''\n",
    "        Function to preprocess bureau_balance table.\n",
    "        This function first loads the table into memory, does some feature engineering, and finally\n",
    "        aggregates the data over SK_ID_BUREAU\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            preprocessed and aggregated bureau_balance table.\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('#######################################################')\n",
    "            print('#          Pre-processing bureau_balance.csv          #')\n",
    "            print('#######################################################')\n",
    "            print(\"\\nLoading the DataFrame, bureau_balance.csv, into memory...\")\n",
    "\n",
    "        bureau_balance = pd.read_csv('dseb63_final_project_DP_dataset/dseb63_bureau_balance.csv')\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Loaded bureau_balance.csv\")\n",
    "            print(f\"Time Taken to load = {datetime.now() - self.start}\")\n",
    "            print(\"\\nStarting Data Cleaning and Feature Engineering...\")\n",
    "\n",
    "        #as we saw from EDA, bureau_balance has a variable called STATUS, which describes about the status of loan.\n",
    "        #it has 7 labels, we will label encode them\n",
    "        #so we give C as 0, and rest increasing\n",
    "        #also we will give X the benefit of doubt and keep it as middle value\n",
    "        dict_for_status = { 'C': 0, '0': 1, '1': 2, '2': 3, 'X': 4, '3': 5, '4': 6, '5': 7}\n",
    "        bureau_balance['STATUS'] = bureau_balance['STATUS'].map(dict_for_status)\n",
    "\n",
    "        #weighing the status with the months_balance \n",
    "        #converting months to positive\n",
    "        bureau_balance['MONTHS_BALANCE'] = np.abs(bureau_balance['MONTHS_BALANCE'])\n",
    "        bureau_balance['WEIGHTED_STATUS'] = bureau_balance.STATUS / (bureau_balance.MONTHS_BALANCE + 1)\n",
    "\n",
    "        #sorting the bureau_balance in ascending order of month and by the bureau SK_ID\n",
    "        #this is done so as to make the rolling exponential average easily for previous months till current month\n",
    "        bureau_balance = bureau_balance.sort_values(by=['SK_ID_BUREAU', 'MONTHS_BALANCE'], ascending=[0, 0])\n",
    "        #we will do exponential weighted average on the encoded status\n",
    "        #this is because if a person had a bad status 2 years ago, it should be given less weightage today\n",
    "        # we keep the latent variable alpha = 0.8 \n",
    "        #doing this for both weighted status and the status itself \n",
    "        bureau_balance['EXP_WEIGHTED_STATUS'] = bureau_balance.groupby('SK_ID_BUREAU')['WEIGHTED_STATUS'].transform(lambda x: x.ewm(alpha = 0.8).mean())\n",
    "        bureau_balance['EXP_ENCODED_STATUS'] = bureau_balance.groupby('SK_ID_BUREAU')['STATUS'].transform(lambda x: x.ewm(alpha = 0.8).mean())    \n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Halfway through. A little bit more patience...\")\n",
    "            print(f\"Total Time Elapsed = {datetime.now() - self.start}\")\n",
    "\n",
    "        #we can see that these datapoints are for 96 months i.e. 8 years.\n",
    "        #so we will extract the means, and exponential averages for each year separately\n",
    "        #first we convert month to year\n",
    "        bureau_balance['MONTHS_BALANCE'] = bureau_balance['MONTHS_BALANCE'] // 12\n",
    "\n",
    "        #defining our aggregations\n",
    "        aggregations_basic = {\n",
    "            'MONTHS_BALANCE' : ['mean','max'],\n",
    "            'STATUS' : ['mean','max','first'],\n",
    "            'WEIGHTED_STATUS' : ['mean','sum','first'],\n",
    "            'EXP_ENCODED_STATUS' : ['last'],\n",
    "            'EXP_WEIGHTED_STATUS' : ['last']}\n",
    "\n",
    "        #we will be finding aggregates for each year too\n",
    "        aggregations_for_year = {\n",
    "            'STATUS' : ['mean','max','last','first'],\n",
    "            'WEIGHTED_STATUS' : ['mean','max', 'first','last'],\n",
    "            'EXP_WEIGHTED_STATUS' : ['last'],\n",
    "            'EXP_ENCODED_STATUS' : ['last'] }\n",
    "\n",
    "        #aggregating over whole dataset first\n",
    "        aggregated_bureau_balance = bureau_balance.groupby(['SK_ID_BUREAU']).agg(aggregations_basic)\n",
    "        aggregated_bureau_balance.columns = ['_'.join(ele).upper() for ele in aggregated_bureau_balance.columns]\n",
    "\n",
    "        #aggregating some of the features separately for latest 2 years\n",
    "        aggregated_bureau_years = pd.DataFrame()\n",
    "        for year in range(2):\n",
    "            year_group = bureau_balance[bureau_balance['MONTHS_BALANCE'] == year].groupby('SK_ID_BUREAU').agg(aggregations_for_year)\n",
    "            year_group.columns = ['_'.join(ele).upper() + '_YEAR_' + str(year) for ele in year_group.columns]\n",
    "\n",
    "            if year == 0:\n",
    "                aggregated_bureau_years = year_group\n",
    "            else:\n",
    "                aggregated_bureau_years = aggregated_bureau_years.merge(year_group, on = 'SK_ID_BUREAU', how = 'outer')\n",
    "\n",
    "        #aggregating for rest of the years\n",
    "        aggregated_bureau_rest_years = bureau_balance[bureau_balance.MONTHS_BALANCE > year].groupby(['SK_ID_BUREAU']).agg(aggregations_for_year)\n",
    "        aggregated_bureau_rest_years.columns = ['_'.join(ele).upper() + '_YEAR_REST' for ele in aggregated_bureau_rest_years.columns]\n",
    "\n",
    "        #merging with rest of the years\n",
    "        aggregated_bureau_years = aggregated_bureau_years.merge(aggregated_bureau_rest_years, on = 'SK_ID_BUREAU', how = 'outer')\n",
    "        aggregated_bureau_balance = aggregated_bureau_balance.merge(aggregated_bureau_years, on = 'SK_ID_BUREAU', how = 'inner')\n",
    "\n",
    "        #filling the missing values obtained after aggregations with 0\n",
    "        aggregated_bureau_balance.fillna(0, inplace = True)\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Done preprocessing bureau_balance.')\n",
    "            print(f\"\\nInitial Size of bureau_balance: {bureau_balance.shape}\")\n",
    "            print(f'Size of bureau_balance after Pre-Processing, Feature Engineering and Aggregation: {aggregated_bureau_balance.shape}')\n",
    "            print(f'\\nTotal Time Taken = {datetime.now() - self.start}')\n",
    "\n",
    "        if self.dump_to_pickle:\n",
    "            if self.verbose:\n",
    "                print('\\nPickling pre-processed bureau_balance to bureau_balance_preprocessed.pkl')\n",
    "            with open(self.file_directory + 'bureau_balance_preprocessed.pkl', 'wb') as f:\n",
    "                pickle.dump(aggregated_bureau_balance, f)\n",
    "            if self.verbose:\n",
    "                print('Done.')     \n",
    "        \n",
    "        return aggregated_bureau_balance\n",
    "    \n",
    "    def preprocess_bureau(self, aggregated_bureau_balance):\n",
    "        '''\n",
    "        Function to preprocess the bureau table and merge it with the aggregated bureau_balance table.\n",
    "        Finally aggregates the data over SK_ID_CURR for it to be merged with application_train table.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            aggregated_bureau_balance: DataFrame of aggregated bureau_balance table\n",
    "        \n",
    "        Returns:\n",
    "            Final preprocessed, merged and aggregated bureau table\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            start2 = datetime.now()\n",
    "            print('\\n##############################################')\n",
    "            print('#          Pre-processing bureau.csv         #')\n",
    "            print('##############################################')\n",
    "            print(\"\\nLoading the DataFrame, bureau.csv, into memory...\")\n",
    "\n",
    "        bureau = pd.read_csv('dseb63_final_project_DP_dataset/dseb63_bureau.csv')\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Loaded bureau.csv\")\n",
    "            print(f\"Time Taken to load = {datetime.now() - start2}\")\n",
    "            print(\"\\nStarting Data Cleaning and Feature Engineering...\")\n",
    "\n",
    "        #merging it with aggregated bureau_balance on 'SK_ID_BUREAU'\n",
    "        bureau_merged = bureau.merge(aggregated_bureau_balance, on = 'SK_ID_BUREAU', how = 'left')\n",
    "\n",
    "        #from the EDA we saw some erroneous values in DAYS Fields, we will remove those\n",
    "        #there are some loans which ended about very long ago, around 100 years ago.\n",
    "        #Thus we will only keep those loans which have ended in past 50 years.\n",
    "        bureau_merged['DAYS_CREDIT_ENDDATE'][bureau_merged['DAYS_CREDIT_ENDDATE'] > -50*365] = np.nan\n",
    "        bureau_merged['DAYS_ENDDATE_FACT'][bureau_merged['DAYS_ENDDATE_FACT'] > -50*365] = np.nan\n",
    "        #there is also a feature which tells about the number of days ago the Credit Report Came\n",
    "        bureau_merged['DAYS_CREDIT_UPDATE'][bureau_merged['DAYS_CREDIT_UPDATE'] > -50*365] = np.nan\n",
    "        \n",
    "        #engineering some features based on domain knowledge\n",
    "        bureau_merged['CREDIT_DURATION'] = np.abs(bureau_merged['DAYS_CREDIT'] - bureau_merged['DAYS_CREDIT_ENDDATE'])\n",
    "        bureau_merged['FLAG_OVERDUE_RECENT'] = [0 if ele == 0 else 1 for ele in bureau_merged['CREDIT_DAY_OVERDUE']]\n",
    "        bureau_merged['MAX_AMT_OVERDUE_DURATION_RATIO'] = bureau_merged['AMT_CREDIT_MAX_OVERDUE'] / (bureau_merged['CREDIT_DURATION'] + 0.00001)\n",
    "        bureau_merged['CURRENT_AMT_OVERDUE_DURATION_RATIO'] = bureau_merged['AMT_CREDIT_SUM_OVERDUE'] / (bureau_merged['CREDIT_DURATION'] + 0.00001)\n",
    "        bureau_merged['AMT_OVERDUE_DURATION_LEFT_RATIO'] = bureau_merged['AMT_CREDIT_SUM_OVERDUE'] / (bureau_merged['DAYS_CREDIT_ENDDATE'] + 0.00001)\n",
    "        bureau_merged['CNT_PROLONGED_MAX_OVERDUE_MUL'] = bureau_merged['CNT_CREDIT_PROLONG'] * bureau_merged['AMT_CREDIT_MAX_OVERDUE']\n",
    "        bureau_merged['CNT_PROLONGED_DURATION_RATIO'] = bureau_merged['CNT_CREDIT_PROLONG'] / (bureau_merged['CREDIT_DURATION'] + 0.00001)\n",
    "        bureau_merged['CURRENT_DEBT_TO_CREDIT_RATIO'] = bureau_merged['AMT_CREDIT_SUM_DEBT'] / (bureau_merged['AMT_CREDIT_SUM'] + 0.00001)\n",
    "        bureau_merged['CURRENT_CREDIT_DEBT_DIFF'] = bureau_merged['AMT_CREDIT_SUM'] - bureau_merged['AMT_CREDIT_SUM_DEBT']\n",
    "        bureau_merged['AMT_ANNUITY_CREDIT_RATIO'] = bureau_merged['AMT_ANNUITY'] / (bureau_merged['AMT_CREDIT_SUM'] + 0.00001)\n",
    "        bureau_merged['CREDIT_ENDDATE_UPDATE_DIFF'] = np.abs(bureau_merged['DAYS_CREDIT_UPDATE'] - bureau_merged['DAYS_CREDIT_ENDDATE'])\n",
    "\n",
    "        #now we will be aggregating the bureau_merged df with respect to 'SK_ID_CURR' so as to merge it with application_train later    \n",
    "        #firstly we will aggregate the columns based on the category of CREDIT_ACTIVE\n",
    "        aggregations_CREDIT_ACTIVE = {\n",
    "                        'DAYS_CREDIT' : ['mean','min','max','last'],\n",
    "                        'CREDIT_DAY_OVERDUE' : ['mean','max'],\n",
    "                        'DAYS_CREDIT_ENDDATE' : ['mean','max'],\n",
    "                        'DAYS_ENDDATE_FACT' : ['mean','min'],\n",
    "                        'AMT_CREDIT_MAX_OVERDUE': ['max','sum'],\n",
    "                        'CNT_CREDIT_PROLONG': ['max','sum'],\n",
    "                        'AMT_CREDIT_SUM' : ['sum','max'],\n",
    "                        'AMT_CREDIT_SUM_DEBT': ['sum'],\n",
    "                        'AMT_CREDIT_SUM_LIMIT': ['max','sum'],\n",
    "                        'AMT_CREDIT_SUM_OVERDUE': ['max','sum'],\n",
    "                        'DAYS_CREDIT_UPDATE' : ['mean','min'],\n",
    "                        'AMT_ANNUITY' : ['mean','sum','max'],\n",
    "                        'CREDIT_DURATION' : ['max','mean'],\n",
    "                        'FLAG_OVERDUE_RECENT': ['sum'],\n",
    "                        'MAX_AMT_OVERDUE_DURATION_RATIO' : ['max','sum'],\n",
    "                        'CURRENT_AMT_OVERDUE_DURATION_RATIO' : ['max','sum'],\n",
    "                        'AMT_OVERDUE_DURATION_LEFT_RATIO' : ['max', 'mean'],\n",
    "                        'CNT_PROLONGED_MAX_OVERDUE_MUL' : ['mean','max'],\n",
    "                        'CNT_PROLONGED_DURATION_RATIO' : ['mean', 'max'],\n",
    "                        'CURRENT_DEBT_TO_CREDIT_RATIO' : ['mean', 'min'],\n",
    "                        'CURRENT_CREDIT_DEBT_DIFF' : ['mean','min'],\n",
    "                        'AMT_ANNUITY_CREDIT_RATIO' : ['mean','max','min'],\n",
    "                        'CREDIT_ENDDATE_UPDATE_DIFF' : ['max','min'],\n",
    "                        'STATUS_MEAN' : ['mean', 'max'],\n",
    "                        'WEIGHTED_STATUS_MEAN' : ['mean', 'max']\n",
    "                         }\n",
    "\n",
    "        # we saw from EDA that the two most common type of CREDIT ACTIVE were 'Closed' and 'Active'.\n",
    "        # So we will aggregate them two separately and the remaining categories separately.\n",
    "        categories_to_aggregate_on = ['Closed','Active']\n",
    "        bureau_merged_aggregated_credit = pd.DataFrame()\n",
    "        for i, status in enumerate(categories_to_aggregate_on):\n",
    "            group = bureau_merged[bureau_merged['CREDIT_ACTIVE'] == status].groupby('SK_ID_CURR').agg(aggregations_CREDIT_ACTIVE)\n",
    "            group.columns = ['_'.join(ele).upper() + '_CREDITACTIVE_' + status.upper() for ele in group.columns]\n",
    "\n",
    "            if i==0:\n",
    "                bureau_merged_aggregated_credit = group\n",
    "            else:\n",
    "                bureau_merged_aggregated_credit = bureau_merged_aggregated_credit.merge(group, on = 'SK_ID_CURR', how = 'outer')\n",
    "        #aggregating for remaining categories\n",
    "        bureau_merged_aggregated_credit_rest = bureau_merged[(bureau_merged['CREDIT_ACTIVE'] != 'Active') & \n",
    "                                                             (bureau_merged['CREDIT_ACTIVE'] != 'Closed')].groupby('SK_ID_CURR').agg(aggregations_CREDIT_ACTIVE)\n",
    "        bureau_merged_aggregated_credit_rest.columns = ['_'.join(ele).upper() + 'CREDIT_ACTIVE_REST' for ele in bureau_merged_aggregated_credit_rest.columns]\n",
    "\n",
    "        #merging with other categories\n",
    "        bureau_merged_aggregated_credit = bureau_merged_aggregated_credit.merge(bureau_merged_aggregated_credit_rest, on = 'SK_ID_CURR', how = 'outer')\n",
    "\n",
    "        #Encoding the categorical columns in one-hot form\n",
    "        currency_ohe = pd.get_dummies(bureau_merged['CREDIT_CURRENCY'], prefix = 'CURRENCY')\n",
    "        credit_active_ohe = pd.get_dummies(bureau_merged['CREDIT_ACTIVE'], prefix = 'CREDIT_ACTIVE')\n",
    "        credit_type_ohe = pd.get_dummies(bureau_merged['CREDIT_TYPE'], prefix = 'CREDIT_TYPE')\n",
    "\n",
    "        #merging the one-hot encoded columns\n",
    "        bureau_merged = pd.concat([bureau_merged.drop(['CREDIT_CURRENCY','CREDIT_ACTIVE','CREDIT_TYPE'], axis = 1), \n",
    "                                   currency_ohe, credit_active_ohe, credit_type_ohe], axis = 1)\n",
    "\n",
    "        #aggregating the bureau_merged over all the columns\n",
    "        bureau_merged_aggregated = bureau_merged.drop('SK_ID_BUREAU', axis = 1).groupby('SK_ID_CURR').agg('mean')\n",
    "        bureau_merged_aggregated.columns = [ele + '_MEAN_OVERALL' for ele in bureau_merged_aggregated.columns]\n",
    "        #merging it with aggregates over categories\n",
    "        bureau_merged_aggregated = bureau_merged_aggregated.merge(bureau_merged_aggregated_credit, on = 'SK_ID_CURR', how = 'outer')\n",
    "\n",
    "        if self.verbose:\n",
    "            print('Done preprocessing bureau and bureau_balance.')\n",
    "            print(f\"\\nInitial Size of bureau: {bureau.shape}\")\n",
    "            print(f'Size of bureau and bureau_balance after Merging, Pre-Processing, Feature Engineering and Aggregation: {bureau_merged_aggregated.shape}')\n",
    "            print(f'\\nTotal Time Taken = {datetime.now() - self.start}')\n",
    "\n",
    "        if self.dump_to_pickle:\n",
    "            if self.verbose:\n",
    "                print('\\nPickling pre-processed bureau and bureau_balance to bureau_merged_preprocessed.pkl')\n",
    "            with open(self.file_directory + 'bureau_merged_preprocessed.pkl', 'wb') as f:\n",
    "                pickle.dump(bureau_merged_aggregated, f)\n",
    "            if self.verbose:\n",
    "                print('Done.')  \n",
    "        if self.verbose:\n",
    "            print('-'*100)\n",
    "\n",
    "        return bureau_merged_aggregated\n",
    "    \n",
    "    def main(self):\n",
    "        '''\n",
    "        Function to be called for complete preprocessing and aggregation of the bureau and bureau_balance tables.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            Final pre=processed and merged bureau and burea_balance tables\n",
    "        '''\n",
    "        \n",
    "        #preprocessing the bureau_balance first\n",
    "        aggregated_bureau_balance = self.preprocess_bureau_balance()\n",
    "        #preprocessing the bureau table next, by combining it with the aggregated bureau_balance\n",
    "        bureau_merged_aggregated = self.preprocess_bureau(aggregated_bureau_balance)\n",
    "        \n",
    "        return bureau_merged_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T07:59:03.155575Z",
     "start_time": "2020-10-23T07:43:46.193002Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "#          Pre-processing bureau_balance.csv          #\n",
      "#######################################################\n",
      "\n",
      "Loading the DataFrame, bureau_balance.csv, into memory...\n",
      "Loaded bureau_balance.csv\n",
      "Time Taken to load = 0:00:03.978172\n",
      "\n",
      "Starting Data Cleaning and Feature Engineering...\n",
      "Halfway through. A little bit more patience...\n",
      "Total Time Elapsed = 0:04:54.756047\n",
      "Done preprocessing bureau_balance.\n",
      "\n",
      "Initial Size of bureau_balance: (27299925, 6)\n",
      "Size of bureau_balance after Pre-Processing, Feature Engineering and Aggregation: (817395, 40)\n",
      "\n",
      "Total Time Taken = 0:05:01.163089\n",
      "\n",
      "Pickling pre-processed bureau_balance to bureau_balance_preprocessed.pkl\n",
      "Done.\n",
      "\n",
      "##############################################\n",
      "#          Pre-processing bureau.csv         #\n",
      "##############################################\n",
      "\n",
      "Loading the DataFrame, bureau.csv, into memory...\n",
      "Loaded bureau.csv\n",
      "Time Taken to load = 0:00:01.492739\n",
      "\n",
      "Starting Data Cleaning and Feature Engineering...\n",
      "Done preprocessing bureau and bureau_balance.\n",
      "\n",
      "Initial Size of bureau: (1465325, 17)\n",
      "Size of bureau and bureau_balance after Merging, Pre-Processing, Feature Engineering and Aggregation: (263491, 242)\n",
      "\n",
      "Total Time Taken = 0:05:10.122049\n",
      "\n",
      "Pickling pre-processed bureau and bureau_balance to bureau_merged_preprocessed.pkl\n",
      "Done.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "bureau_aggregated = preprocess_bureau_balance_and_bureau(dump_to_pickle = True).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### previous_application.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table contains the static data related to clients and their previous credits with Home Credit Group.\n",
    "<ol><li>First we start by cleaning the erroneous values. From the EDA we saw some DAYS fields with a value equal to 365243.0, they look erroneous, and so we will be replacing them with NaN values. </li>\n",
    "    <li>We replace the NaN values for categories with an 'XNA' category.</li>\n",
    "    <li>Next, we proceed to feature engineering, where we create some domain based features, such as Credit-Downpayment Ratio, Amount not approved, Credit to Goods ratio, etc.</li>\n",
    "    <li>We also try to predict the interest rate, inspired by one of the writeups of winners. </li>\n",
    "    <li>To be able to merge it with main table, we need to aggregate the rows of previous_application over SK_ID_CURR. We perform domain based aggregations, over all the previous credits for each customer, such as mean, max, min, etc. Here again we aggregate in three ways. First we perform overall aggregation, next we aggregate for first 2 applications and latest 5 applications. The First and Last are decided by the DAYS_FIRST_DUE of applications. In the end, we merge all these aggregations together.</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T07:59:03.236359Z",
     "start_time": "2020-10-23T07:59:03.159590Z"
    }
   },
   "outputs": [],
   "source": [
    "class preprocess_previous_application:\n",
    "    '''\n",
    "    Preprocess the previous_application table.\n",
    "    Contains 5 member functions:\n",
    "        1. init method\n",
    "        2. load_dataframe method\n",
    "        3. data_cleaning method\n",
    "        4. preprocessing_feature_engineering method\n",
    "        5. main method\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, file_directory = '', verbose = True, dump_to_pickle = False):\n",
    "        '''\n",
    "        This function is used to initialize the class members \n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            file_directory: Path, str, default = ''\n",
    "                The path where the file exists. Include a '/' at the end of the path in input\n",
    "            verbose: bool, default = True\n",
    "                Whether to enable verbosity or not\n",
    "            dump_to_pickle: bool, default = False\n",
    "                Whether to pickle the final preprocessed table or not\n",
    "                \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        self.file_directory = file_directory\n",
    "        self.verbose = verbose\n",
    "        self.dump_to_pickle = dump_to_pickle\n",
    "    \n",
    "    def load_dataframe(self):\n",
    "        '''\n",
    "        Function to load the previous_application.csv DataFrame.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.start = datetime.now()\n",
    "            print('########################################################')\n",
    "            print('#        Pre-processing previous_application.csv        #')\n",
    "            print('########################################################')\n",
    "            print(\"\\nLoading the DataFrame, previous_application.csv, into memory...\")\n",
    "\n",
    "        #loading the DataFrame into memory\n",
    "        self.previous_application = pd.read_csv('dseb63_final_project_DP_dataset/dseb63_previous_application.csv')\n",
    "        self.initial_shape = self.previous_application.shape\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Loaded previous_application.csv\")\n",
    "            print(f\"Time Taken to load = {datetime.now() - self.start}\")\n",
    "    \n",
    "    def data_cleaning(self):\n",
    "        '''\n",
    "        Function to clean the data. Removes erroneous points, fills categorical NaNs with 'XNA'.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            start = datetime.now()\n",
    "            print('\\nStarting Data Cleaning...')\n",
    "            \n",
    "        #sorting the applications from oldest to most recent previous loans for each user\n",
    "        self.previous_application = self.previous_application.sort_values(by = ['SK_ID_CURR','DAYS_FIRST_DUE'])\n",
    "        \n",
    "        #in the EDA we found some erroneous values in DAYS columns, so we will replace them with NaN values\n",
    "        self.previous_application['DAYS_FIRST_DRAWING'][self.previous_application['DAYS_FIRST_DRAWING'] == 365243.0] = np.nan\n",
    "        self.previous_application['DAYS_FIRST_DUE'][self.previous_application['DAYS_FIRST_DUE'] == 365243.0] = np.nan\n",
    "        self.previous_application['DAYS_LAST_DUE_1ST_VERSION'][self.previous_application['DAYS_LAST_DUE_1ST_VERSION'] == 365243.0] = np.nan\n",
    "        self.previous_application['DAYS_LAST_DUE'][self.previous_application['DAYS_LAST_DUE'] == 365243.0] = np.nan\n",
    "        self.previous_application['DAYS_TERMINATION'][self.previous_application['DAYS_TERMINATION'] == 365243.0] = np.nan\n",
    "        #we also see abruptly large value for SELLERPLACE_AREA\n",
    "        self.previous_application['SELLERPLACE_AREA'][self.previous_application['SELLERPLACE_AREA'] == 4000000] = np.nan\n",
    "        #filling the NaN values for categories\n",
    "        categorical_columns = self.previous_application.dtypes[self.previous_application.dtypes == 'object'].index.tolist()\n",
    "        self.previous_application[categorical_columns] = self.previous_application[categorical_columns].fillna('XNA')\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Done.\")\n",
    "            print(f\"Time taken = {datetime.now() - start}\")\n",
    "\n",
    "    def preprocessing_feature_engineering(self):\n",
    "        '''\n",
    "        Function to do preprocessing such as categorical encoding and feature engineering.\n",
    "        \n",
    "        Inputs: \n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            start = datetime.now()\n",
    "            print(\"\\nPerforming Preprocessing and Feature Engineering...\")\n",
    "\n",
    "        #label encoding the categorical variables\n",
    "        name_contract_dict = {'Approved': 0, 'Refused' : 3, 'Canceled' : 2, 'Unused offer' : 1}\n",
    "        self.previous_application['NAME_CONTRACT_STATUS'] = self.previous_application['NAME_CONTRACT_STATUS'].map(name_contract_dict)\n",
    "        yield_group_dict = {'XNA': 0, 'low_action': 1, 'low_normal': 2,'middle': 3, 'high': 4}\n",
    "        self.previous_application['NAME_YIELD_GROUP'] = self.previous_application['NAME_YIELD_GROUP'].map(yield_group_dict)\n",
    "        appl_per_contract_last_dict = {'Y':1, 'N':0}\n",
    "        self.previous_application['FLAG_LAST_APPL_PER_CONTRACT'] = self.previous_application['FLAG_LAST_APPL_PER_CONTRACT'].map(appl_per_contract_last_dict)\n",
    "        remaining_categorical_columns = self.previous_application.dtypes[self.previous_application.dtypes == 'object'].index.tolist()\n",
    "        for col in remaining_categorical_columns:\n",
    "            encoding_dict = dict([(j,i) for i,j in enumerate(self.previous_application[col].unique(),1)])\n",
    "            self.previous_application[col] = self.previous_application[col].map(encoding_dict)    \n",
    "        \n",
    "        #engineering some features on domain knowledge\n",
    "        self.previous_application['MISSING_VALUES_TOTAL_PREV'] = self.previous_application.isna().sum(axis = 1)\n",
    "        self.previous_application['AMT_DECLINED'] = self.previous_application['AMT_APPLICATION'] - self.previous_application['AMT_CREDIT']\n",
    "        self.previous_application['AMT_CREDIT_GOODS_RATIO'] = self.previous_application['AMT_CREDIT'] / (self.previous_application['AMT_GOODS_PRICE'] + 0.00001)\n",
    "        self.previous_application['AMT_CREDIT_GOODS_DIFF'] = self.previous_application['AMT_CREDIT'] - self.previous_application['AMT_GOODS_PRICE']\n",
    "        self.previous_application['AMT_CREDIT_APPLICATION_RATIO'] = self.previous_application['AMT_APPLICATION'] / (self.previous_application['AMT_CREDIT'] + 0.00001)\n",
    "        self.previous_application['CREDIT_DOWNPAYMENT_RATIO'] = self.previous_application['AMT_DOWN_PAYMENT'] / (self.previous_application['AMT_CREDIT'] + 0.00001)\n",
    "        self.previous_application['GOOD_DOWNPAYMET_RATIO'] = self.previous_application['AMT_DOWN_PAYMENT'] / (self.previous_application['AMT_GOODS_PRICE'] + 0.00001)\n",
    "        self.previous_application['INTEREST_DOWNPAYMENT'] = self.previous_application['RATE_DOWN_PAYMENT'] * self.previous_application['AMT_DOWN_PAYMENT']\n",
    "        self.previous_application['INTEREST_CREDIT'] = self.previous_application['AMT_CREDIT'] * self.previous_application['RATE_INTEREST_PRIMARY']\n",
    "        self.previous_application['INTEREST_CREDIT_PRIVILEGED'] = self.previous_application['AMT_CREDIT'] * self.previous_application['RATE_INTEREST_PRIVILEGED']\n",
    "        self.previous_application['APPLICATION_AMT_TO_DECISION_RATIO'] = self.previous_application['AMT_APPLICATION'] / (self.previous_application['DAYS_DECISION'] + 0.00001) * -1\n",
    "        self.previous_application['AMT_APPLICATION_TO_SELLERPLACE_AREA'] = self.previous_application['AMT_APPLICATION'] / (self.previous_application['SELLERPLACE_AREA'] + 0.00001)\n",
    "        self.previous_application['ANNUITY'] = self.previous_application['AMT_CREDIT'] / (self.previous_application['CNT_PAYMENT'] + 0.00001)\n",
    "        self.previous_application['ANNUITY_GOODS'] = self.previous_application['AMT_GOODS_PRICE'] / (self.previous_application['CNT_PAYMENT'] + 0.00001)\n",
    "        self.previous_application['DAYS_FIRST_LAST_DUE_DIFF' ] = self.previous_application['DAYS_LAST_DUE'] - self.previous_application['DAYS_FIRST_DUE']\n",
    "        self.previous_application['AMT_CREDIT_HOUR_PROCESS_START'] = self.previous_application['AMT_CREDIT'] * self.previous_application['HOUR_APPR_PROCESS_START']\n",
    "        self.previous_application['AMT_CREDIT_NFLAG_LAST_APPL_DAY'] = self.previous_application['AMT_CREDIT'] * self.previous_application['NFLAG_LAST_APPL_IN_DAY']\n",
    "        self.previous_application['AMT_CREDIT_YIELD_GROUP'] = self.previous_application['AMT_CREDIT'] * self.previous_application['NAME_YIELD_GROUP']\n",
    "        #https://www.kaggle.com/c/home-credit-default-risk/discussion/64598\n",
    "        self.previous_application['AMT_INTEREST'] = self.previous_application['CNT_PAYMENT'] * self.previous_application[\n",
    "                                                'AMT_ANNUITY'] - self.previous_application['AMT_CREDIT'] \n",
    "        self.previous_application['INTEREST_SHARE'] = self.previous_application['AMT_INTEREST'] / (self.previous_application[\n",
    "                                                                                                'AMT_CREDIT'] + 0.00001)\n",
    "        self.previous_application['INTEREST_RATE'] = 2 * 12 * self.previous_application['AMT_INTEREST'] / (self.previous_application[\n",
    "                                            'AMT_CREDIT'] * (self.previous_application['CNT_PAYMENT'] + 1))\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Done.\")\n",
    "            print(f\"Time taken = {datetime.now() - start}\")\n",
    "    \n",
    "    def aggregations(self):\n",
    "        '''\n",
    "        Function to aggregate the previous applications over SK_ID_CURR\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "        \n",
    "        Returns:\n",
    "            aggregated previous_applications\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\nAggregating previous applications over SK_ID_CURR...\")\n",
    "            \n",
    "        aggregations_for_previous_application = {\n",
    "            'MISSING_VALUES_TOTAL_PREV' : ['sum'],\n",
    "            'NAME_CONTRACT_TYPE' : ['mean','last'],\n",
    "            'AMT_ANNUITY' : ['mean','sum','max'],\n",
    "            'AMT_APPLICATION' : ['mean','max','sum'],\n",
    "            'AMT_CREDIT' : ['mean','max','sum'],\n",
    "            'AMT_DOWN_PAYMENT' : ['mean','max','sum'],\n",
    "            'AMT_GOODS_PRICE' : ['mean','max','sum'],\n",
    "            'WEEKDAY_APPR_PROCESS_START' : ['mean','max','min'],\n",
    "            'HOUR_APPR_PROCESS_START' : ['mean','max','min'],\n",
    "            'FLAG_LAST_APPL_PER_CONTRACT' : ['mean','sum'],\n",
    "            'NFLAG_LAST_APPL_IN_DAY' : ['mean','sum'],\n",
    "            'RATE_DOWN_PAYMENT' : ['mean','max'],\n",
    "            'RATE_INTEREST_PRIMARY' : ['mean','max'],\n",
    "            'RATE_INTEREST_PRIVILEGED' : ['mean','max'],\n",
    "            'NAME_CASH_LOAN_PURPOSE' : ['mean','last'],\n",
    "            'NAME_CONTRACT_STATUS' : ['mean','max','last'],\n",
    "            'DAYS_DECISION' : ['mean','max','min'],\n",
    "            'NAME_PAYMENT_TYPE' : ['mean', 'last'],\n",
    "            'CODE_REJECT_REASON' : ['mean','last'],\n",
    "            'NAME_TYPE_SUITE' : ['mean','last'],\n",
    "            'NAME_CLIENT_TYPE' : ['mean','last'],\n",
    "            'NAME_GOODS_CATEGORY' : ['mean','last'],\n",
    "            'NAME_PORTFOLIO' : ['mean','last'],\n",
    "            'NAME_PRODUCT_TYPE' : ['mean','last'],\n",
    "            'CHANNEL_TYPE' : ['mean','last'],\n",
    "            'SELLERPLACE_AREA' : ['mean','max','min'],\n",
    "            'NAME_SELLER_INDUSTRY' : ['mean','last'],\n",
    "            'CNT_PAYMENT' : ['sum','mean','max'],\n",
    "            'NAME_YIELD_GROUP' : ['mean','last'],\n",
    "            'PRODUCT_COMBINATION' : ['mean', 'last'],\n",
    "            'DAYS_FIRST_DRAWING' : ['mean','max'],\n",
    "            'DAYS_FIRST_DUE' : ['mean','max'],\n",
    "            'DAYS_LAST_DUE_1ST_VERSION' : ['mean'],\n",
    "            'DAYS_LAST_DUE' : ['mean'],\n",
    "            'DAYS_TERMINATION' : ['mean','max'],\n",
    "            'NFLAG_INSURED_ON_APPROVAL' : ['sum'],\n",
    "            'AMT_DECLINED' : ['mean','max','sum'],\n",
    "            'AMT_CREDIT_GOODS_RATIO' : ['mean', 'max', 'min'],\n",
    "            'AMT_CREDIT_GOODS_DIFF' : ['sum','mean','max', 'min'],\n",
    "            'AMT_CREDIT_APPLICATION_RATIO' : ['mean','min'],\n",
    "            'CREDIT_DOWNPAYMENT_RATIO' : ['mean','max'],\n",
    "            'GOOD_DOWNPAYMET_RATIO' : ['mean','max'],\n",
    "            'INTEREST_DOWNPAYMENT' : ['mean','sum','max'],\n",
    "            'INTEREST_CREDIT' : ['mean','sum','max'],\n",
    "            'INTEREST_CREDIT_PRIVILEGED' : ['mean','sum','max'],\n",
    "            'APPLICATION_AMT_TO_DECISION_RATIO' : ['mean','min'],\n",
    "            'AMT_APPLICATION_TO_SELLERPLACE_AREA' : ['mean','max'],\n",
    "            'ANNUITY' : ['mean','sum','max'],\n",
    "            'ANNUITY_GOODS' : ['mean','sum','max'],\n",
    "            'DAYS_FIRST_LAST_DUE_DIFF' : ['mean','max'],\n",
    "            'AMT_CREDIT_HOUR_PROCESS_START' : ['mean','sum'],\n",
    "            'AMT_CREDIT_NFLAG_LAST_APPL_DAY' : ['mean','max'],\n",
    "            'AMT_CREDIT_YIELD_GROUP' : ['mean','sum','min'],\n",
    "            'AMT_INTEREST' : ['mean','sum','max','min'],\n",
    "            'INTEREST_SHARE' : ['mean','max','min'],\n",
    "            'INTEREST_RATE' : ['mean','max','min'] \n",
    "        }\n",
    "\n",
    "        #grouping the previous applications over SK_ID_CURR while only taking the latest 5 applications\n",
    "        group_last_3 = self.previous_application.groupby('SK_ID_CURR').tail(5).groupby('SK_ID_CURR').agg(aggregations_for_previous_application)\n",
    "        group_last_3.columns = ['_'.join(ele).upper() + '_LAST_5' for ele in group_last_3.columns]\n",
    "        #grouping the previous applications over SK_ID_CURR while only taking the first 2 applications\n",
    "        group_first_3 = self.previous_application.groupby('SK_ID_CURR').head(2).groupby('SK_ID_CURR').agg(aggregations_for_previous_application)\n",
    "        group_first_3.columns = ['_'.join(ele).upper() + '_FIRST_2' for ele in group_first_3.columns]\n",
    "        #grouping the previous applications over SK_ID_CURR while taking all the applications into consideration\n",
    "        group_all = self.previous_application.groupby('SK_ID_CURR').agg(aggregations_for_previous_application)\n",
    "        group_all.columns = ['_'.join(ele).upper() + '_ALL' for ele in group_all.columns]\n",
    "\n",
    "        #merging all the applications\n",
    "        previous_application_aggregated = group_last_3.merge(group_first_3, on = 'SK_ID_CURR', how = 'outer')\n",
    "        previous_application_aggregated = previous_application_aggregated.merge(group_all, on = 'SK_ID_CURR', how = 'outer')\n",
    "\n",
    "        return previous_application_aggregated\n",
    "    \n",
    "    def main(self):\n",
    "        '''\n",
    "        Function to be called for complete preprocessing and aggregation of previous_application table.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            Final pre=processed and aggregated previous_application table.\n",
    "        '''\n",
    "        \n",
    "        #loading the DataFrame\n",
    "        self.load_dataframe()\n",
    "        \n",
    "        #cleaning the data\n",
    "        self.data_cleaning()\n",
    "        \n",
    "        #preprocessing the categorical features and creating new features\n",
    "        self.preprocessing_feature_engineering()\n",
    "        \n",
    "        #aggregating data over SK_ID_CURR\n",
    "        previous_application_aggregated = self.aggregations()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Done aggregations.')\n",
    "            print(f\"\\nInitial Size of previous_application: {self.initial_shape}\")\n",
    "            print(f'Size of previous_application after Pre-Processing, Feature Engineering and Aggregation: {previous_application_aggregated.shape}')\n",
    "            print(f'\\nTotal Time Taken = {datetime.now() - self.start}')\n",
    "\n",
    "        if self.dump_to_pickle:\n",
    "            if self.verbose:\n",
    "                print('\\nPickling pre-processed previous_application to previous_application_preprocessed.pkl')\n",
    "            with open(self.file_directory + 'previous_application_preprocessed.pkl', 'wb') as f:\n",
    "                pickle.dump(previous_application_aggregated, f)\n",
    "            if self.verbose:\n",
    "                print('Done.')  \n",
    "        if self.verbose:\n",
    "            print('-'*100)\n",
    "                    \n",
    "        return previous_application_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T07:59:57.118744Z",
     "start_time": "2020-10-23T07:59:03.240350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################\n",
      "#        Pre-processing previous_application.csv        #\n",
      "########################################################\n",
      "\n",
      "Loading the DataFrame, previous_application.csv, into memory...\n",
      "Loaded previous_application.csv\n",
      "Time Taken to load = 0:00:03.876551\n",
      "\n",
      "Starting Data Cleaning...\n",
      "Done.\n",
      "Time taken = 0:00:02.157231\n",
      "\n",
      "Performing Preprocessing and Feature Engineering...\n",
      "Done.\n",
      "Time taken = 0:00:02.266578\n",
      "\n",
      "Aggregating previous applications over SK_ID_CURR...\n",
      "Done aggregations.\n",
      "\n",
      "Initial Size of previous_application: (1413701, 37)\n",
      "Size of previous_application after Pre-Processing, Feature Engineering and Aggregation: (291057, 399)\n",
      "\n",
      "Total Time Taken = 0:00:13.600604\n",
      "\n",
      "Pickling pre-processed previous_application to previous_application_preprocessed.pkl\n",
      "Done.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "previous_aggregated = preprocess_previous_application(dump_to_pickle = True).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T06:09:07.460554Z",
     "start_time": "2020-10-10T06:09:07.455601Z"
    }
   },
   "source": [
    "### installments_payments.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table contains the details about each installment of client's previous credits with Home Credit Group.\n",
    "<ol><li>We start by sorting the data first by SK_ID_CURR and SK_ID_PREV, and then by NUM_INSTALMENT_NUMBER. This brings the latest installments in the end.</li>\n",
    "    <li>We create some features, such as the number of days the payment was delayed, the difference in amount of payment required vs paid, etc.</li>\n",
    "    <li>Next we aggregate these rows over SK_ID_PREV, such that each client's previous loan gets one row. These aggregations are done in three ways, first overall aggregations, second we aggregate only those installments which were in the last 365 days, and lastly, we aggregate the first 5 installments of every loan. This will help us to capture the starting behaviour, the latest behaviour and the overall behaviour of the client's installments payments.</li>\n",
    "    <li>Now to merge this table with main table, we aggregate the data over SK_ID_CURR.</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T07:59:57.155157Z",
     "start_time": "2020-10-23T07:59:57.122229Z"
    }
   },
   "outputs": [],
   "source": [
    "class preprocess_installments_payments:\n",
    "    '''\n",
    "    Preprocess the installments_payments table.\n",
    "    Contains 6 member functions:\n",
    "        1. init method\n",
    "        2. load_dataframe method\n",
    "        3. data_preprocessing_and_feature_engineering method\n",
    "        4. aggregations_sk_id_prev method\n",
    "        5. aggregations_sk_id_curr method\n",
    "        6. main method\n",
    "    '''\n",
    "     \n",
    "    def __init__(self, file_directory = '', verbose = True, dump_to_pickle = False):\n",
    "        '''\n",
    "        This function is used to initialize the class members \n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            file_directory: Path, str, default = ''\n",
    "                The path where the file exists. Include a '/' at the end of the path in input\n",
    "            verbose: bool, default = True\n",
    "                Whether to enable verbosity or not\n",
    "            dump_to_pickle: bool, default = False\n",
    "                Whether to pickle the final preprocessed table or not\n",
    "                \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        self.file_directory = file_directory\n",
    "        self.verbose = verbose\n",
    "        self.dump_to_pickle = dump_to_pickle\n",
    "        \n",
    "    def load_dataframe(self):\n",
    "        '''\n",
    "        Function to load the installments_payments.csv DataFrame.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.start = datetime.now()\n",
    "            print('##########################################################')\n",
    "            print('#        Pre-processing installments_payments.csv        #')\n",
    "            print('##########################################################')\n",
    "            print(\"\\nLoading the DataFrame, installments_payments.csv, into memory...\")\n",
    "\n",
    "        self.installments_payments = pd.read_csv('dseb63_final_project_DP_dataset/dseb63_installments_payments.csv')\n",
    "        self.initial_shape = self.installments_payments.shape\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Loaded previous_application.csv\")\n",
    "            print(f\"Time Taken to load = {datetime.now() - self.start}\")\n",
    "\n",
    "    def data_preprocessing_and_feature_engineering(self):\n",
    "        '''\n",
    "        Function for pre-processing and feature engineering\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            start = datetime.now()\n",
    "            print(\"\\nStarting Data Pre-processing and Feature Engineering...\")\n",
    "        \n",
    "        #sorting by SK_ID_PREV and NUM_INSTALMENT_NUMBER\n",
    "        self.installments_payments = self.installments_payments.sort_values(by = ['SK_ID_CURR','SK_ID_PREV','NUM_INSTALMENT_NUMBER'], ascending = True)\n",
    "        \n",
    "        #getting the total NaN values in the table\n",
    "        self.installments_payments['MISSING_VALS_TOTAL_INSTAL'] = self.installments_payments.isna().sum(axis = 1)\n",
    "        #engineering new features based on some domain based polynomial operations\n",
    "        self.installments_payments['DAYS_PAYMENT_RATIO'] = self.installments_payments['DAYS_INSTALMENT'] / (self.installments_payments['DAYS_ENTRY_PAYMENT'] + 0.00001)\n",
    "        self.installments_payments['DAYS_PAYMENT_DIFF'] = self.installments_payments['DAYS_INSTALMENT'] - self.installments_payments['DAYS_ENTRY_PAYMENT']\n",
    "        self.installments_payments['AMT_PAYMENT_RATIO'] = self.installments_payments['AMT_PAYMENT'] / (self.installments_payments['AMT_INSTALMENT'] + 0.00001)\n",
    "        self.installments_payments['AMT_PAYMENT_DIFF'] = self.installments_payments['AMT_INSTALMENT'] - self.installments_payments['AMT_PAYMENT']\n",
    "        self.installments_payments['EXP_DAYS_PAYMENT_RATIO'] = self.installments_payments['DAYS_PAYMENT_RATIO'].transform(lambda x: x.ewm(alpha = 0.5).mean())\n",
    "        self.installments_payments['EXP_DAYS_PAYMENT_DIFF'] = self.installments_payments['DAYS_PAYMENT_DIFF'].transform(lambda x: x.ewm(alpha = 0.5).mean())\n",
    "        self.installments_payments['EXP_AMT_PAYMENT_RATIO'] = self.installments_payments['AMT_PAYMENT_RATIO'].transform(lambda x: x.ewm(alpha = 0.5).mean())\n",
    "        self.installments_payments['EXP_AMT_PAYMENT_DIFF'] = self.installments_payments['AMT_PAYMENT_DIFF'].transform(lambda x: x.ewm(alpha = 0.5).mean())\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Done.\")\n",
    "            print(f\"Time Taken = {datetime.now() - start}\")\n",
    "    \n",
    "    def aggregations_sk_id_prev(self):\n",
    "        '''\n",
    "        Function for aggregations of installments on previous loans over SK_ID_PREV\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "        \n",
    "        Returns:\n",
    "            installments_payments table aggregated over previous loans\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            start = datetime.now()\n",
    "            print(\"\\nPerforming Aggregations over SK_ID_PREV...\")\n",
    "        \n",
    "        #aggregating the data over SK_ID_PREV, i.e. for each previous loan\n",
    "        overall_aggregations = {\n",
    "            'MISSING_VALS_TOTAL_INSTAL' : ['sum'],\n",
    "            'NUM_INSTALMENT_VERSION' : ['mean','sum'],\n",
    "            'NUM_INSTALMENT_NUMBER' : ['max'],\n",
    "            'DAYS_INSTALMENT' : ['max','min'],\n",
    "            'DAYS_ENTRY_PAYMENT' : ['max','min'],\n",
    "            'AMT_INSTALMENT' : ['mean', 'sum', 'max'],\n",
    "            'AMT_PAYMENT' : ['mean', 'sum', 'max'],\n",
    "            'DAYS_PAYMENT_RATIO' : ['mean', 'min','max'],\n",
    "            'DAYS_PAYMENT_DIFF' : ['mean','min','max'],\n",
    "            'AMT_PAYMENT_RATIO' : ['mean','min','max'],\n",
    "            'AMT_PAYMENT_DIFF' : ['mean','min','max'],\n",
    "            'EXP_DAYS_PAYMENT_RATIO' : ['last'],\n",
    "            'EXP_DAYS_PAYMENT_DIFF' : ['last'],\n",
    "            'EXP_AMT_PAYMENT_RATIO' : ['last'],\n",
    "            'EXP_AMT_PAYMENT_DIFF' : ['last']\n",
    "        }\n",
    "        limited_period_aggregations = {\n",
    "            'NUM_INSTALMENT_VERSION' : ['mean','sum'],\n",
    "            'AMT_INSTALMENT' : ['mean', 'sum', 'max'],\n",
    "            'AMT_PAYMENT' : ['mean', 'sum', 'max'],\n",
    "            'DAYS_PAYMENT_RATIO' : ['mean', 'min','max'],\n",
    "            'DAYS_PAYMENT_DIFF' : ['mean','min','max'],\n",
    "            'AMT_PAYMENT_RATIO' : ['mean','min','max'],\n",
    "            'AMT_PAYMENT_DIFF' : ['mean','min','max'],\n",
    "            'EXP_DAYS_PAYMENT_RATIO' : ['last'],\n",
    "            'EXP_DAYS_PAYMENT_DIFF' : ['last'],\n",
    "            'EXP_AMT_PAYMENT_RATIO' : ['last'],\n",
    "            'EXP_AMT_PAYMENT_DIFF' : ['last']\n",
    "        }\n",
    "\n",
    "        #aggregating installments_payments over SK_ID_PREV for last 1 year installments\n",
    "        group_last_1_year = self.installments_payments[self.installments_payments['DAYS_INSTALMENT'] > -365].groupby('SK_ID_PREV').agg(limited_period_aggregations)\n",
    "        group_last_1_year.columns = ['_'.join(ele).upper() + '_LAST_1_YEAR' for ele in group_last_1_year.columns]\n",
    "        #aggregating installments_payments over SK_ID_PREV for first 5 installments\n",
    "        group_first_5_instalments = self.installments_payments.groupby('SK_ID_PREV', as_index = False).head(5).groupby('SK_ID_PREV').agg(limited_period_aggregations)\n",
    "        group_first_5_instalments.columns = ['_'.join(ele).upper() + '_FIRST_5_INSTALLMENTS' for ele in group_first_5_instalments.columns]\n",
    "        #overall aggregation of installments_payments over SK_ID_PREV\n",
    "        group_overall = self.installments_payments.groupby(['SK_ID_PREV','SK_ID_CURR'], as_index = False).agg(overall_aggregations)\n",
    "        group_overall.columns = ['_'.join(ele).upper() for ele in group_overall.columns]\n",
    "        group_overall.rename(columns = {'SK_ID_PREV_': 'SK_ID_PREV','SK_ID_CURR_' : 'SK_ID_CURR'}, inplace = True)\n",
    "\n",
    "        #merging all of the above aggregations together\n",
    "        installments_payments_agg_prev = group_overall.merge(group_last_1_year, on = 'SK_ID_PREV', how = 'outer')\n",
    "        installments_payments_agg_prev = installments_payments_agg_prev.merge(group_first_5_instalments, on = 'SK_ID_PREV', how = 'outer')\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Done.\")\n",
    "            print(f\"Time Taken = {datetime.now() - start}\")\n",
    "            \n",
    "        return installments_payments_agg_prev\n",
    "    \n",
    "    def aggregations_sk_id_curr(self, installments_payments_agg_prev):\n",
    "        '''\n",
    "        Function to aggregate the installments payments on previous loans over SK_ID_CURR\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            installments_payments_agg_prev: DataFrame\n",
    "                installments payments aggregated over SK_ID_PREV\n",
    "        \n",
    "        Returns:\n",
    "            installments payments aggregated over SK_ID_CURR\n",
    "        '''\n",
    "                    \n",
    "        #aggregating over SK_ID_CURR\n",
    "        main_features_aggregations = {\n",
    "            'MISSING_VALS_TOTAL_INSTAL_SUM' : ['sum'],\n",
    "            'NUM_INSTALMENT_VERSION_MEAN' : ['mean'],\n",
    "            'NUM_INSTALMENT_VERSION_SUM' : ['mean'],\n",
    "            'NUM_INSTALMENT_NUMBER_MAX' : ['mean','sum','max'],\n",
    "            'AMT_INSTALMENT_MEAN' : ['mean','sum','max'],\n",
    "            'AMT_INSTALMENT_SUM' : ['mean','sum','max'],\n",
    "            'AMT_INSTALMENT_MAX' : ['mean'],\n",
    "            'AMT_PAYMENT_MEAN' : ['mean','sum','max'],\n",
    "            'AMT_PAYMENT_SUM' : ['mean','sum','max'],\n",
    "            'AMT_PAYMENT_MAX' : ['mean'],\n",
    "            'DAYS_PAYMENT_RATIO_MEAN' : ['mean','min','max'],\n",
    "            'DAYS_PAYMENT_RATIO_MIN' : ['mean','min'],\n",
    "            'DAYS_PAYMENT_RATIO_MAX' : ['mean','max'],\n",
    "            'DAYS_PAYMENT_DIFF_MEAN' : ['mean','min','max'],\n",
    "            'DAYS_PAYMENT_DIFF_MIN' : ['mean','min'],\n",
    "            'DAYS_PAYMENT_DIFF_MAX' : ['mean','max'],\n",
    "            'AMT_PAYMENT_RATIO_MEAN' : ['mean', 'min','max'],\n",
    "            'AMT_PAYMENT_RATIO_MIN' : ['mean','min'],\n",
    "            'AMT_PAYMENT_RATIO_MAX' : ['mean','max'],\n",
    "            'AMT_PAYMENT_DIFF_MEAN' : ['mean','min','max'],\n",
    "            'AMT_PAYMENT_DIFF_MIN' : ['mean','min'],\n",
    "            'AMT_PAYMENT_DIFF_MAX' : ['mean','max'],\n",
    "            'EXP_DAYS_PAYMENT_RATIO_LAST' : ['mean'],\n",
    "            'EXP_DAYS_PAYMENT_DIFF_LAST' : ['mean'],\n",
    "            'EXP_AMT_PAYMENT_RATIO_LAST' : ['mean'],\n",
    "            'EXP_AMT_PAYMENT_DIFF_LAST' : ['mean']\n",
    "        }\n",
    "\n",
    "        grouped_main_features = installments_payments_agg_prev.groupby('SK_ID_CURR').agg(main_features_aggregations)\n",
    "        grouped_main_features.columns = ['_'.join(ele).upper() for ele in grouped_main_features.columns]\n",
    "\n",
    "        #group remaining ones\n",
    "        grouped_remaining_features = installments_payments_agg_prev.iloc[:,[1] + list(range(31,len(installments_payments_agg_prev.columns)))].groupby('SK_ID_CURR').mean()\n",
    "\n",
    "        installments_payments_aggregated = grouped_main_features.merge(grouped_remaining_features, on = 'SK_ID_CURR', how = 'inner')\n",
    "                \n",
    "        return installments_payments_aggregated\n",
    "    \n",
    "    def main(self):\n",
    "        '''\n",
    "        Function to be called for complete preprocessing and aggregation of installments_payments table.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            Final pre=processed and aggregated installments_payments table.\n",
    "        '''\n",
    "        \n",
    "        #loading the dataframe\n",
    "        self.load_dataframe()\n",
    "        #doing pre-processing and feature engineering\n",
    "        self.data_preprocessing_and_feature_engineering()\n",
    "        #First aggregating the data for each SK_ID_PREV\n",
    "        installments_payments_agg_prev = self.aggregations_sk_id_prev()\n",
    "    \n",
    "        if self.verbose:\n",
    "            print(\"\\nAggregations over SK_ID_CURR...\")\n",
    "        #aggregating the previous loans for each SK_ID_CURR\n",
    "        installments_payments_aggregated = self.aggregations_sk_id_curr(installments_payments_agg_prev)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('\\nDone preprocessing installments_payments.')\n",
    "            print(f\"\\nInitial Size of installments_payments: {self.initial_shape}\")\n",
    "            print(f'Size of installments_payments after Pre-Processing, Feature Engineering and Aggregation: {installments_payments_aggregated.shape}')\n",
    "            print(f'\\nTotal Time Taken = {datetime.now() - self.start}')\n",
    "\n",
    "        if self.dump_to_pickle:\n",
    "            if self.verbose:\n",
    "                print('\\nPickling pre-processed installments_payments to installments_payments_preprocessed.pkl')\n",
    "            with open(self.file_directory + 'installments_payments_preprocessed.pkl', 'wb') as f:\n",
    "                pickle.dump(installments_payments_aggregated, f)\n",
    "            if self.verbose:\n",
    "                print('Done.')  \n",
    "        if self.verbose:\n",
    "            print('-'*100)\n",
    "\n",
    "        return installments_payments_aggregated   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T08:00:51.720586Z",
     "start_time": "2020-10-23T07:59:57.158229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################\n",
      "#        Pre-processing installments_payments.csv        #\n",
      "##########################################################\n",
      "\n",
      "Loading the DataFrame, installments_payments.csv, into memory...\n",
      "Loaded previous_application.csv\n",
      "Time Taken to load = 0:00:04.087903\n",
      "\n",
      "Starting Data Pre-processing and Feature Engineering...\n",
      "Done.\n",
      "Time Taken = 0:00:05.177898\n",
      "\n",
      "Performing Aggregations over SK_ID_PREV...\n",
      "Done.\n",
      "Time Taken = 0:00:07.538423\n",
      "\n",
      "Aggregations over SK_ID_CURR...\n",
      "\n",
      "Done preprocessing installments_payments.\n",
      "\n",
      "Initial Size of installments_payments: (7744758, 8)\n",
      "Size of installments_payments after Pre-Processing, Feature Engineering and Aggregation: (180733, 101)\n",
      "\n",
      "Total Time Taken = 0:00:18.251849\n",
      "\n",
      "Pickling pre-processed installments_payments to installments_payments_preprocessed.pkl\n",
      "Done.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "installments_aggregated = preprocess_installments_payments(dump_to_pickle = True).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS_CASH_balance.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table contains the Monthly Balance Snapshots of previous Point of Sales and Cash Loans that the applicant had with Home Credit Group. The table contains columns like the status of contract, the number of installments left, etc.\n",
    "\n",
    "<ol><li>Similar to bureau_balance table, this table also has time based features. So we start off by computing the EDAs on CNT_INSTALMENT and CNT_INSTALMENT_FUTURE features. </li>\n",
    "    <li>We create some domain based features next.</li>\n",
    "    <li>We then aggregate the data over SK_ID_PREV. For this aggregation, we do it in 3 ways. Firstly we aggregate the whole data over SK_ID_PREV. We also aggregate the data for last 2 years separately and rest of the years separately. Finally, we also aggregate the data different Contract types, i.e. Active and Completed.</li>\n",
    "    <li>Next, we aggregate the data over SK_ID_CURR, for it to be merged with main table.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T08:00:51.761519Z",
     "start_time": "2020-10-23T08:00:51.722579Z"
    }
   },
   "outputs": [],
   "source": [
    "class preprocess_POS_CASH_balance:\n",
    "    '''\n",
    "    Preprocess the POS_CASH_balance table.\n",
    "    Contains 6 member functions:\n",
    "        1. init method\n",
    "        2. load_dataframe method\n",
    "        3. data_preprocessing_and_feature_engineering method\n",
    "        4. aggregations_sk_id_prev method\n",
    "        5. aggregations_sk_id_curr method\n",
    "        6. main method\n",
    "    '''\n",
    "\n",
    "    def __init__(self, file_directory = '', verbose = True, dump_to_pickle = False):\n",
    "        '''\n",
    "        This function is used to initialize the class members \n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            file_directory: Path, str, default = ''\n",
    "                The path where the file exists. Include a '/' at the end of the path in input\n",
    "            verbose: bool, default = True\n",
    "                Whether to enable verbosity or not\n",
    "            dump_to_pickle: bool, default = False\n",
    "                Whether to pickle the final preprocessed table or not\n",
    "                \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        self.file_directory = file_directory\n",
    "        self.verbose = verbose\n",
    "        self.dump_to_pickle = dump_to_pickle\n",
    "    \n",
    "    def load_dataframe(self):\n",
    "        '''\n",
    "        Function to load the POS_CASH_balance.csv DataFrame.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.start = datetime.now()\n",
    "            print('#########################################################')\n",
    "            print('#          Pre-processing POS_CASH_balance.csv          #')\n",
    "            print('#########################################################')\n",
    "            print(\"\\nLoading the DataFrame, POS_CASH_balance.csv, into memory...\")\n",
    "\n",
    "        self.pos_cash = pd.read_csv('dseb63_final_project_DP_dataset/dseb63_POS_CASH_balance.csv')\n",
    "        self.initial_size = self.pos_cash.shape\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Loaded POS_CASH_balance.csv\")\n",
    "            print(f\"Time Taken to load = {datetime.now() - self.start}\")\n",
    "            \n",
    "    def data_preprocessing_and_feature_engineering(self):\n",
    "        '''\n",
    "        Function to preprocess the table and create new features.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            start = datetime.now()\n",
    "            print(\"\\nStarting Data Cleaning and Feature Engineering...\")\n",
    "\n",
    "        #making the MONTHS_BALANCE Positive\n",
    "        self.pos_cash['MONTHS_BALANCE'] = np.abs(self.pos_cash['MONTHS_BALANCE'])\n",
    "        #sorting the DataFrame according to the month of status from oldest to latest, for rolling computations \n",
    "        self.pos_cash = self.pos_cash.sort_values(by=['SK_ID_PREV', 'MONTHS_BALANCE'], ascending=False)\n",
    "\n",
    "        #computing Exponential Moving Average for some features based on MONTHS_BALANCE\n",
    "        columns_for_ema = ['CNT_INSTALMENT', 'CNT_INSTALMENT_FUTURE']\n",
    "        exp_columns = ['EXP_'+ele for ele in columns_for_ema]\n",
    "        self.pos_cash[exp_columns] = self.pos_cash.groupby('SK_ID_PREV')[columns_for_ema].transform(lambda x: x.ewm(alpha = 0.6).mean())\n",
    "\n",
    "        #creating new features based on Domain Knowledge\n",
    "        self.pos_cash['SK_DPD_RATIO'] = self.pos_cash['SK_DPD'] / (self.pos_cash['SK_DPD_DEF'] + 0.00001)\n",
    "        self.pos_cash['TOTAL_TERM'] = self.pos_cash['CNT_INSTALMENT'] + self.pos_cash['CNT_INSTALMENT_FUTURE']\n",
    "        self.pos_cash['EXP_POS_TOTAL_TERM'] = self.pos_cash['EXP_CNT_INSTALMENT'] + self.pos_cash['EXP_CNT_INSTALMENT_FUTURE']\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Done.\")\n",
    "            print(f\"Time Taken = {datetime.now() - start}\")\n",
    "            \n",
    "    def aggregations_sk_id_prev(self):\n",
    "        '''\n",
    "        Function to aggregated the POS_CASH_balance rows over SK_ID_PREV\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "        \n",
    "        Returns:\n",
    "            Aggregated POS_CASH_balance table over SK_ID_PREV\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            start = datetime.now()\n",
    "            print(\"\\nAggregations over SK_ID_PREV...\")\n",
    "            \n",
    "        #aggregating over SK_ID_PREV\n",
    "        overall_aggregations = {\n",
    "            'SK_ID_CURR' : ['first'],\n",
    "            'MONTHS_BALANCE' : ['max'],\n",
    "            'CNT_INSTALMENT' : ['mean', 'max','min'],\n",
    "            'CNT_INSTALMENT_FUTURE' : ['mean','max','min'],\n",
    "            'SK_DPD' : ['max','sum'],\n",
    "            'SK_DPD_DEF' : ['max','sum'],\n",
    "            'EXP_CNT_INSTALMENT' : ['last'],\n",
    "            'EXP_CNT_INSTALMENT_FUTURE' : ['last'],\n",
    "            'SK_DPD_RATIO' : ['mean','max'],\n",
    "            'TOTAL_TERM' : ['mean','max','last'],\n",
    "            'EXP_POS_TOTAL_TERM' : ['mean'] \n",
    "        }\n",
    "        aggregations_for_year = {\n",
    "            'CNT_INSTALMENT' : ['mean', 'max','min'],\n",
    "            'CNT_INSTALMENT_FUTURE' : ['mean','max','min'],\n",
    "            'SK_DPD' : ['max','sum'],\n",
    "            'SK_DPD_DEF' : ['max','sum'],\n",
    "            'EXP_CNT_INSTALMENT' : ['last'],\n",
    "            'EXP_CNT_INSTALMENT_FUTURE' : ['last'],\n",
    "            'SK_DPD_RATIO' : ['mean','max'],\n",
    "            'TOTAL_TERM' : ['mean','max'],\n",
    "            'EXP_POS_TOTAL_TERM' : ['last'] \n",
    "        }\n",
    "        aggregations_for_categories = {\n",
    "            'CNT_INSTALMENT' : ['mean', 'max','min'],\n",
    "            'CNT_INSTALMENT_FUTURE' : ['mean','max','min'],\n",
    "            'SK_DPD' : ['max','sum'],\n",
    "            'SK_DPD_DEF' : ['max','sum'],\n",
    "            'EXP_CNT_INSTALMENT' : ['last'],\n",
    "            'EXP_CNT_INSTALMENT_FUTURE' : ['last'],\n",
    "            'SK_DPD_RATIO' : ['mean','max'],\n",
    "            'TOTAL_TERM' : ['mean','max'],\n",
    "            'EXP_POS_TOTAL_TERM' : ['last']\n",
    "        }\n",
    "        #performing overall aggregations over SK_ID_PREV\n",
    "        pos_cash_aggregated_overall = self.pos_cash.groupby('SK_ID_PREV').agg(overall_aggregations)\n",
    "        pos_cash_aggregated_overall.columns = ['_'.join(ele).upper() for ele in pos_cash_aggregated_overall.columns]\n",
    "        pos_cash_aggregated_overall.rename(columns = {'SK_ID_CURR_FIRST': 'SK_ID_CURR'}, inplace = True)\n",
    "\n",
    "        #yearwise aggregations\n",
    "        self.pos_cash['YEAR_BALANCE'] = self.pos_cash['MONTHS_BALANCE'] //12\n",
    "        #aggregating over SK_ID_PREV for each last 2 years\n",
    "        pos_cash_aggregated_year = pd.DataFrame()\n",
    "        for year in range(2):\n",
    "            group = self.pos_cash[self.pos_cash['YEAR_BALANCE'] == year].groupby('SK_ID_PREV').agg(aggregations_for_year)\n",
    "            group.columns = ['_'.join(ele).upper() + '_YEAR_' + str(year) for ele in group.columns]\n",
    "            if year == 0:\n",
    "                pos_cash_aggregated_year = group\n",
    "            else:\n",
    "                pos_cash_aggregated_year = pos_cash_aggregated_year.merge(group, on = 'SK_ID_PREV', how = 'outer')\n",
    "\n",
    "        #aggregating over SK_ID_PREV for rest of the years\n",
    "        pos_cash_aggregated_rest_years = self.pos_cash[self.pos_cash['YEAR_BALANCE'] >= 2].groupby('SK_ID_PREV').agg(aggregations_for_year)\n",
    "        pos_cash_aggregated_rest_years.columns = ['_'.join(ele).upper() + '_YEAR_REST' for ele in pos_cash_aggregated_rest_years.columns]\n",
    "        #merging all the years aggregations\n",
    "        pos_cash_aggregated_year = pos_cash_aggregated_year.merge(pos_cash_aggregated_rest_years, on = 'SK_ID_PREV', how = 'outer')\n",
    "        self.pos_cash = self.pos_cash.drop(['YEAR_BALANCE'], axis = 1)\n",
    "\n",
    "        #aggregating over SK_ID_PREV for each of NAME_CONTRACT_STATUS categories\n",
    "        contract_type_categories = ['Active', 'Completed']\n",
    "        pos_cash_aggregated_contract = pd.DataFrame()\n",
    "        for i, contract_type in enumerate(contract_type_categories):\n",
    "            group = self.pos_cash[self.pos_cash['NAME_CONTRACT_STATUS'] == contract_type].groupby('SK_ID_PREV').agg(aggregations_for_categories)\n",
    "            group.columns = ['_'.join(ele).upper() + '_' + contract_type.upper() for ele in group.columns]\n",
    "            if i == 0:\n",
    "                pos_cash_aggregated_contract = group\n",
    "            else:\n",
    "                pos_cash_aggregated_contract = pos_cash_aggregated_contract.merge(group, on = 'SK_ID_PREV', how = 'outer')\n",
    "\n",
    "        pos_cash_aggregated_rest_contract = self.pos_cash[(self.pos_cash['NAME_CONTRACT_STATUS'] != 'Active') & \n",
    "                                        (self.pos_cash['NAME_CONTRACT_STATUS'] != 'Completed')].groupby('SK_ID_PREV').agg(aggregations_for_categories)\n",
    "        pos_cash_aggregated_rest_contract.columns = ['_'.join(ele).upper() + '_REST' for ele in pos_cash_aggregated_rest_contract.columns]\n",
    "        #merging the categorical aggregations\n",
    "        pos_cash_aggregated_contract = pos_cash_aggregated_contract.merge(pos_cash_aggregated_rest_contract, on = 'SK_ID_PREV', how = 'outer')    \n",
    "\n",
    "        #merging all the aggregations\n",
    "        pos_cash_aggregated = pos_cash_aggregated_overall.merge(pos_cash_aggregated_year, on = 'SK_ID_PREV', how = 'outer')\n",
    "        pos_cash_aggregated = pos_cash_aggregated.merge(pos_cash_aggregated_contract, on = 'SK_ID_PREV', how = 'outer')\n",
    "\n",
    "        #onehot encoding the categorical feature NAME_CONTRACT_TYPE\n",
    "        name_contract_dummies = pd.get_dummies(self.pos_cash['NAME_CONTRACT_STATUS'], prefix='CONTRACT')\n",
    "        contract_names = name_contract_dummies.columns.tolist()\n",
    "        #concatenating one-hot encoded categories with main table\n",
    "        self.pos_cash = pd.concat([self.pos_cash, name_contract_dummies], axis=1)\n",
    "        #aggregating these over SK_ID_PREV as well\n",
    "        aggregated_cc_contract = self.pos_cash[['SK_ID_PREV'] + contract_names].groupby('SK_ID_PREV').mean()    \n",
    "\n",
    "        #merging with the final aggregations\n",
    "        pos_cash_aggregated = pos_cash_aggregated.merge(aggregated_cc_contract, on = 'SK_ID_PREV', how = 'outer')\n",
    "                \n",
    "        if self.verbose:\n",
    "            print(\"Done.\")\n",
    "            print(f\"Time Taken = {datetime.now() - start}\")\n",
    "        \n",
    "        return pos_cash_aggregated\n",
    "\n",
    "    def aggregations_sk_id_curr(self, pos_cash_aggregated):\n",
    "        '''\n",
    "        Function to aggregated the aggregateed POS_CASH_balance table over SK_ID_CURR\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            pos_cash_aggregated: DataFrame\n",
    "                aggregated pos_cash table over SK_ID_PREV\n",
    "                \n",
    "        Returns:\n",
    "            pos_cash_balance table aggregated over SK_ID_CURR\n",
    "        '''\n",
    "        \n",
    "        #aggregating over SK_ID_CURR\n",
    "        columns_to_aggregate = pos_cash_aggregated.columns[1:]\n",
    "        #defining the aggregations to perform\n",
    "        aggregations_final = {}\n",
    "        for col in columns_to_aggregate:\n",
    "            if 'MEAN' in col:\n",
    "                aggregates = ['mean','sum','max']\n",
    "            else:\n",
    "                aggregates = ['mean']\n",
    "            aggregations_final[col] = aggregates\n",
    "        pos_cash_aggregated_final = pos_cash_aggregated.groupby('SK_ID_CURR').agg(aggregations_final)\n",
    "        pos_cash_aggregated_final.columns = ['_'.join(ele).upper() for ele in pos_cash_aggregated_final.columns]\n",
    "        \n",
    "        return pos_cash_aggregated_final\n",
    "    \n",
    "    def main(self):\n",
    "        '''\n",
    "        Function to be called for complete preprocessing and aggregation of POS_CASH_balance table.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            Final pre=processed and aggregated POS_CASH_balance table.\n",
    "        '''\n",
    "        \n",
    "        #loading the dataframe\n",
    "        self.load_dataframe()\n",
    "        #performing the data pre-processing and feature engineering\n",
    "        self.data_preprocessing_and_feature_engineering()\n",
    "        #performing aggregations over SK_ID_PREV\n",
    "        pos_cash_aggregated = self.aggregations_sk_id_prev()\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\nAggregation over SK_ID_CURR...\")\n",
    "        #doing aggregations over each SK_ID_CURR\n",
    "        pos_cash_aggregated_final = self.aggregations_sk_id_curr(pos_cash_aggregated)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('\\nDone preprocessing POS_CASH_balance.')\n",
    "            print(f\"\\nInitial Size of POS_CASH_balance: {self.initial_size}\")\n",
    "            print(f'Size of POS_CASH_balance after Pre-Processing, Feature Engineering and Aggregation: {pos_cash_aggregated_final.shape}')\n",
    "            print(f'\\nTotal Time Taken = {datetime.now() - self.start}')\n",
    "\n",
    "        if self.dump_to_pickle:\n",
    "            if self.verbose:\n",
    "                print('\\nPickling pre-processed POS_CASH_balance to POS_CASH_balance_preprocessed.pkl')\n",
    "            with open(self.file_directory + 'POS_CASH_balance_preprocessed.pkl', 'wb') as f:\n",
    "                pickle.dump(pos_cash_aggregated_final, f)\n",
    "            if self.verbose:\n",
    "                print('Done.')\n",
    "        if self.verbose:\n",
    "            print('-'*100)\n",
    "\n",
    "        return pos_cash_aggregated_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T08:38:07.598681Z",
     "start_time": "2020-10-23T08:00:51.763469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################\n",
      "#          Pre-processing POS_CASH_balance.csv          #\n",
      "#########################################################\n",
      "\n",
      "Loading the DataFrame, POS_CASH_balance.csv, into memory...\n",
      "Loaded POS_CASH_balance.csv\n",
      "Time Taken to load = 0:00:03.588608\n",
      "\n",
      "Starting Data Cleaning and Feature Engineering...\n",
      "Done.\n",
      "Time Taken = 0:02:52.592156\n",
      "\n",
      "Aggregations over SK_ID_PREV...\n",
      "Done.\n",
      "Time Taken = 0:00:11.831062\n",
      "\n",
      "Aggregation over SK_ID_CURR...\n",
      "\n",
      "Done preprocessing POS_CASH_balance.\n",
      "\n",
      "Initial Size of POS_CASH_balance: (8543375, 8)\n",
      "Size of POS_CASH_balance after Pre-Processing, Feature Engineering and Aggregation: (289444, 188)\n",
      "\n",
      "Total Time Taken = 0:03:11.765257\n",
      "\n",
      "Pickling pre-processed POS_CASH_balance to POS_CASH_balance_preprocessed.pkl\n",
      "Done.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pos_aggregated = preprocess_POS_CASH_balance(dump_to_pickle = True).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-10T05:46:44.267150Z",
     "start_time": "2020-10-10T05:46:44.262163Z"
    }
   },
   "source": [
    "### credit_card_balance.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table contains information about the previous credit cards that the client had with Home Credit Group.\n",
    "\n",
    "<ol><li>We start off with removing an erroneous value, and then we proceed to feature engineering.</li>\n",
    "    <li>We create some domain based features such as total drawings, number of drawings, balance to limit ratio, payment done to minimum payment required difference, etc.</li>\n",
    "    <li>This table also contains all these data monthwise, so we calculate the EDAs for some of the features of this table too.</li>\n",
    "    <li>For aggregations, we first aggregate over SK_ID_PREV. Here we aggregate on three bases. Firstly, we do overall aggregations. We also do aggregations for last 2 years separately and the rest of the years. Finally we aggregate over SK_ID_PREV for categorical variable NAME_CONTRACT_TYPE. </li>\n",
    "    <li>For aggregation over SK_ID_CURR, we saw from the EDA that most of the current clients just had 1 credit card previously, so we do simple mean aggregations over SK_ID_CURR.</li></ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T08:38:07.651630Z",
     "start_time": "2020-10-23T08:38:07.602707Z"
    }
   },
   "outputs": [],
   "source": [
    "class preprocess_credit_card_balance:\n",
    "    '''\n",
    "    Preprocess the credit_card_balance table.\n",
    "    Contains 5 member functions:\n",
    "        1. init method\n",
    "        2. load_dataframe method\n",
    "        3. data_preprocessing_and_feature_engineering method\n",
    "        4. aggregations method\n",
    "        5. main method\n",
    "    '''\n",
    "\n",
    "    def __init__(self, file_directory = '', verbose = True, dump_to_pickle = False):\n",
    "        '''\n",
    "        This function is used to initialize the class members \n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            file_directory: Path, str, default = ''\n",
    "                The path where the file exists. Include a '/' at the end of the path in input\n",
    "            verbose: bool, default = True\n",
    "                Whether to enable verbosity or not\n",
    "            dump_to_pickle: bool, default = False\n",
    "                Whether to pickle the final preprocessed table or not\n",
    "                \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        self.file_directory = file_directory\n",
    "        self.verbose = verbose\n",
    "        self.dump_to_pickle = dump_to_pickle\n",
    "    \n",
    "    def load_dataframe(self):\n",
    "        '''\n",
    "        Function to load the credit_card_balance.csv DataFrame.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.start = datetime.now()\n",
    "            print('#########################################################')\n",
    "            print('#        Pre-processing credit_card_balance.csv         #')\n",
    "            print('#########################################################')\n",
    "            print(\"\\nLoading the DataFrame, credit_card_balance.csv, into memory...\")\n",
    "\n",
    "        self.cc_balance = pd.read_csv('dseb63_final_project_DP_dataset/dseb63_credit_card_balance.csv')\n",
    "        self.initial_size = self.cc_balance.shape\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Loaded credit_card_balance.csv\")\n",
    "            print(f\"Time Taken to load = {datetime.now() - self.start}\")\n",
    "            \n",
    "    def data_preprocessing_and_feature_engineering(self):\n",
    "        '''\n",
    "        Function to preprocess the table, by removing erroneous points, and then creating new domain based features.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            start = datetime.now()\n",
    "            print(\"\\nStarting Preprocessing and Feature Engineering...\")\n",
    "            \n",
    "        #there is one abruptly large value for AMT_PAYMENT_CURRENT\n",
    "        self.cc_balance['AMT_PAYMENT_CURRENT'][self.cc_balance['AMT_PAYMENT_CURRENT'] > 4000000] = np.nan\n",
    "        #calculating the total missing values for each previous credit card\n",
    "        self.cc_balance['MISSING_VALS_TOTAL_CC'] = self.cc_balance.isna().sum(axis = 1)\n",
    "        #making the MONTHS_BALANCE Positive\n",
    "        self.cc_balance['MONTHS_BALANCE'] = np.abs(self.cc_balance['MONTHS_BALANCE'])\n",
    "        #sorting the DataFrame according to the month of status from oldest to latest, for rolling computations\n",
    "        self.cc_balance = self.cc_balance.sort_values(by = ['SK_ID_PREV','MONTHS_BALANCE'], ascending = [1,0])\n",
    "\n",
    "        #Creating new features\n",
    "        self.cc_balance['AMT_DRAWING_SUM'] = self.cc_balance['AMT_DRAWINGS_ATM_CURRENT'] + self.cc_balance['AMT_DRAWINGS_CURRENT'] + self.cc_balance[\n",
    "                                    'AMT_DRAWINGS_OTHER_CURRENT'] + self.cc_balance['AMT_DRAWINGS_POS_CURRENT']\n",
    "        self.cc_balance['BALANCE_LIMIT_RATIO'] = self.cc_balance['AMT_BALANCE'] / (self.cc_balance['AMT_CREDIT_LIMIT_ACTUAL'] + 0.00001)\n",
    "        self.cc_balance['CNT_DRAWING_SUM'] = self.cc_balance['CNT_DRAWINGS_ATM_CURRENT'] + self.cc_balance['CNT_DRAWINGS_CURRENT'] + self.cc_balance[\n",
    "                                            'CNT_DRAWINGS_OTHER_CURRENT'] + self.cc_balance['CNT_DRAWINGS_POS_CURRENT'] + self.cc_balance['CNT_INSTALMENT_MATURE_CUM']\n",
    "        self.cc_balance['MIN_PAYMENT_RATIO'] = self.cc_balance['AMT_PAYMENT_CURRENT'] / (self.cc_balance['AMT_INST_MIN_REGULARITY'] + 0.0001)\n",
    "        self.cc_balance['PAYMENT_MIN_DIFF'] = self.cc_balance['AMT_PAYMENT_CURRENT'] - self.cc_balance['AMT_INST_MIN_REGULARITY']\n",
    "        self.cc_balance['MIN_PAYMENT_TOTAL_RATIO'] = self.cc_balance['AMT_PAYMENT_TOTAL_CURRENT'] / (self.cc_balance['AMT_INST_MIN_REGULARITY'] +0.00001)\n",
    "        self.cc_balance['PAYMENT_MIN_DIFF'] = self.cc_balance['AMT_PAYMENT_TOTAL_CURRENT'] - self.cc_balance['AMT_INST_MIN_REGULARITY']\n",
    "        self.cc_balance['AMT_INTEREST_RECEIVABLE'] = self.cc_balance['AMT_TOTAL_RECEIVABLE'] - self.cc_balance['AMT_RECEIVABLE_PRINCIPAL']\n",
    "        self.cc_balance['SK_DPD_RATIO'] = self.cc_balance['SK_DPD'] / (self.cc_balance['SK_DPD_DEF'] + 0.00001)\n",
    "        \n",
    "        #calculating the rolling Exponential Weighted Moving Average over months for certain features\n",
    "        rolling_columns = [\n",
    "            'AMT_BALANCE',\n",
    "            'AMT_CREDIT_LIMIT_ACTUAL',\n",
    "            'AMT_RECEIVABLE_PRINCIPAL',\n",
    "            'AMT_RECIVABLE',\n",
    "            'AMT_TOTAL_RECEIVABLE',\n",
    "            'AMT_DRAWING_SUM',\n",
    "            'BALANCE_LIMIT_RATIO',\n",
    "            'CNT_DRAWING_SUM',\n",
    "            'MIN_PAYMENT_RATIO',\n",
    "            'PAYMENT_MIN_DIFF',\n",
    "            'MIN_PAYMENT_TOTAL_RATIO',\n",
    "            'AMT_INTEREST_RECEIVABLE',\n",
    "            'SK_DPD_RATIO' ]\n",
    "        exp_weighted_columns = ['EXP_' + ele for ele in rolling_columns]\n",
    "        self.cc_balance[exp_weighted_columns] = self.cc_balance.groupby(['SK_ID_CURR','SK_ID_PREV'])[rolling_columns].transform(lambda x: x.ewm(alpha = 0.7).mean())\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Done.\")\n",
    "            print(f\"Time Taken = {datetime.now() - start}\")\n",
    "            \n",
    "    def aggregations(self):\n",
    "        '''\n",
    "        Function to perform aggregations of rows of credit_card_balance table, first over SK_ID_PREV,\n",
    "        and then over SK_ID_CURR\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "        \n",
    "        Returns:\n",
    "            aggregated credit_card_balance table.\n",
    "        '''\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\nAggregating the DataFrame, first over SK_ID_PREv, then over SK_ID_CURR\")\n",
    "\n",
    "        #performing aggregations over SK_ID_PREV\n",
    "        overall_aggregations = {\n",
    "            'SK_ID_CURR' : ['first'],\n",
    "            'MONTHS_BALANCE': ['max'],\n",
    "            'AMT_BALANCE' : ['sum','mean','max'],\n",
    "            'AMT_CREDIT_LIMIT_ACTUAL' : ['sum','mean','max'],\n",
    "            'AMT_DRAWINGS_ATM_CURRENT' : ['sum','max'],\n",
    "            'AMT_DRAWINGS_CURRENT' : ['sum','max'],\n",
    "            'AMT_DRAWINGS_OTHER_CURRENT' : ['sum','max'],\n",
    "            'AMT_DRAWINGS_POS_CURRENT' : ['sum','max'],\n",
    "            'AMT_INST_MIN_REGULARITY' : ['mean','min','max'],\n",
    "            'AMT_PAYMENT_CURRENT' : ['mean','min','max'],\n",
    "            'AMT_PAYMENT_TOTAL_CURRENT' : ['mean','min','max'],\n",
    "            'AMT_RECEIVABLE_PRINCIPAL' : ['sum','mean','max'],\n",
    "            'AMT_RECIVABLE' : ['sum','mean','max'],\n",
    "            'AMT_TOTAL_RECEIVABLE' : ['sum','mean','max'],\n",
    "            'CNT_DRAWINGS_ATM_CURRENT' : ['sum','max'],\n",
    "            'CNT_DRAWINGS_CURRENT' : ['sum','max'],\n",
    "            'CNT_DRAWINGS_OTHER_CURRENT' : ['sum','max'],\n",
    "            'CNT_DRAWINGS_POS_CURRENT' : ['sum','max'],\n",
    "            'CNT_INSTALMENT_MATURE_CUM' : ['sum','max','min'],\n",
    "            'SK_DPD' : ['sum','max'],\n",
    "            'SK_DPD_DEF' : ['sum','max'],\n",
    "\n",
    "            'AMT_DRAWING_SUM' : ['sum','max'],\n",
    "            'BALANCE_LIMIT_RATIO' : ['mean','max','min'],\n",
    "            'CNT_DRAWING_SUM' : ['sum','max'],\n",
    "            'MIN_PAYMENT_RATIO': ['min','mean'],\n",
    "            'PAYMENT_MIN_DIFF' : ['min','mean'],\n",
    "            'MIN_PAYMENT_TOTAL_RATIO' : ['min','mean'], \n",
    "            'AMT_INTEREST_RECEIVABLE' : ['min','mean'],\n",
    "            'SK_DPD_RATIO' : ['max','mean'],\n",
    "\n",
    "            'EXP_AMT_BALANCE' : ['last'],\n",
    "            'EXP_AMT_CREDIT_LIMIT_ACTUAL' : ['last'],\n",
    "            'EXP_AMT_RECEIVABLE_PRINCIPAL' : ['last'],\n",
    "            'EXP_AMT_RECIVABLE' : ['last'],\n",
    "            'EXP_AMT_TOTAL_RECEIVABLE' : ['last'],\n",
    "            'EXP_AMT_DRAWING_SUM' : ['last'],\n",
    "            'EXP_BALANCE_LIMIT_RATIO' : ['last'],\n",
    "            'EXP_CNT_DRAWING_SUM' : ['last'],\n",
    "            'EXP_MIN_PAYMENT_RATIO' : ['last'],\n",
    "            'EXP_PAYMENT_MIN_DIFF' : ['last'],\n",
    "            'EXP_MIN_PAYMENT_TOTAL_RATIO' : ['last'],\n",
    "            'EXP_AMT_INTEREST_RECEIVABLE' : ['last'],\n",
    "            'EXP_SK_DPD_RATIO' : ['last'],\n",
    "            'MISSING_VALS_TOTAL_CC' : ['sum']\n",
    "        }\n",
    "        aggregations_for_categories = {\n",
    "            'SK_DPD' : ['sum','max'],\n",
    "            'SK_DPD_DEF' : ['sum','max'],\n",
    "            'BALANCE_LIMIT_RATIO' : ['mean','max','min'],\n",
    "            'CNT_DRAWING_SUM' : ['sum','max'],\n",
    "            'MIN_PAYMENT_RATIO': ['min','mean'],\n",
    "            'PAYMENT_MIN_DIFF' : ['min','mean'],\n",
    "            'MIN_PAYMENT_TOTAL_RATIO' : ['min','mean'], \n",
    "            'AMT_INTEREST_RECEIVABLE' : ['min','mean'],\n",
    "            'SK_DPD_RATIO' : ['max','mean'],\n",
    "            'EXP_AMT_DRAWING_SUM' : ['last'],\n",
    "            'EXP_BALANCE_LIMIT_RATIO' : ['last'],\n",
    "            'EXP_CNT_DRAWING_SUM' : ['last'],\n",
    "            'EXP_MIN_PAYMENT_RATIO' : ['last'],\n",
    "            'EXP_PAYMENT_MIN_DIFF' : ['last'],\n",
    "            'EXP_MIN_PAYMENT_TOTAL_RATIO' : ['last'],\n",
    "            'EXP_AMT_INTEREST_RECEIVABLE' : ['last'],\n",
    "            'EXP_SK_DPD_RATIO' : ['last']\n",
    "        }\n",
    "        aggregations_for_year = {\n",
    "            'SK_DPD' : ['sum','max'],\n",
    "            'SK_DPD_DEF' : ['sum','max'],\n",
    "            'BALANCE_LIMIT_RATIO' : ['mean','max','min'],\n",
    "            'CNT_DRAWING_SUM' : ['sum','max'],\n",
    "            'MIN_PAYMENT_RATIO': ['min','mean'],\n",
    "            'PAYMENT_MIN_DIFF' : ['min','mean'],\n",
    "            'MIN_PAYMENT_TOTAL_RATIO' : ['min','mean'], \n",
    "            'AMT_INTEREST_RECEIVABLE' : ['min','mean'],\n",
    "            'SK_DPD_RATIO' : ['max','mean'],\n",
    "            'EXP_AMT_DRAWING_SUM' : ['last'],\n",
    "            'EXP_BALANCE_LIMIT_RATIO' : ['last'],\n",
    "            'EXP_CNT_DRAWING_SUM' : ['last'],\n",
    "            'EXP_MIN_PAYMENT_RATIO' : ['last'],\n",
    "            'EXP_PAYMENT_MIN_DIFF' : ['last'],\n",
    "            'EXP_MIN_PAYMENT_TOTAL_RATIO' : ['last'],\n",
    "            'EXP_AMT_INTEREST_RECEIVABLE' : ['last'],\n",
    "            'EXP_SK_DPD_RATIO' : ['last']\n",
    "        }\n",
    "        #performing overall aggregations over SK_ID_PREV for all features\n",
    "        cc_balance_aggregated_overall = self.cc_balance.groupby('SK_ID_PREV').agg(overall_aggregations)\n",
    "        cc_balance_aggregated_overall.columns = ['_'.join(ele).upper() for ele in cc_balance_aggregated_overall.columns]\n",
    "        cc_balance_aggregated_overall.rename(columns = {'SK_ID_CURR_FIRST' : 'SK_ID_CURR'}, inplace = True)\n",
    "\n",
    "        #aggregating over SK_ID_PREV for different categories\n",
    "        contract_status_categories = ['Active','Completed']\n",
    "        cc_balance_aggregated_categories = pd.DataFrame()\n",
    "        for i, contract_type in enumerate(contract_status_categories):\n",
    "            group = self.cc_balance[self.cc_balance['NAME_CONTRACT_STATUS'] == contract_type].groupby('SK_ID_PREV').agg(aggregations_for_categories)\n",
    "            group.columns = ['_'.join(ele).upper() + '_' + contract_type.upper() for ele in group.columns]\n",
    "            if i == 0:\n",
    "                cc_balance_aggregated_categories = group\n",
    "            else:\n",
    "                cc_balance_aggregated_categories = cc_balance_aggregated_categories.merge(group, on = 'SK_ID_PREV', how = 'outer')\n",
    "        #aggregating over SK_ID_PREV for rest of the categories\n",
    "        cc_balance_aggregated_categories_rest = self.cc_balance[(self.cc_balance['NAME_CONTRACT_STATUS'] != 'Active') & \n",
    "                                        (self.cc_balance.NAME_CONTRACT_STATUS != 'Completed')].groupby('SK_ID_PREV').agg(aggregations_for_categories)\n",
    "        cc_balance_aggregated_categories_rest.columns = ['_'.join(ele).upper() + '_REST' for ele in cc_balance_aggregated_categories_rest.columns]\n",
    "        #merging all the categorical aggregations\n",
    "        cc_balance_aggregated_categories = cc_balance_aggregated_categories.merge(cc_balance_aggregated_categories_rest, on = 'SK_ID_PREV', how = 'outer')\n",
    "\n",
    "        #aggregating over SK_ID_PREV for different years\n",
    "        self.cc_balance['YEAR_BALANCE'] = self.cc_balance['MONTHS_BALANCE'] //12\n",
    "        cc_balance_aggregated_year = pd.DataFrame()\n",
    "        for year in range(2):\n",
    "            group = self.cc_balance[self.cc_balance['YEAR_BALANCE'] == year].groupby('SK_ID_PREV').agg(aggregations_for_year)\n",
    "            group.columns = ['_'.join(ele).upper() + '_YEAR_' + str(year) for ele in group.columns]\n",
    "            if year == 0:\n",
    "                cc_balance_aggregated_year = group\n",
    "            else:\n",
    "                cc_balance_aggregated_year = cc_balance_aggregated_year.merge(group, on = 'SK_ID_PREV', how = 'outer')\n",
    "        #aggregating over SK_ID_PREV for rest of years\n",
    "        cc_balance_aggregated_year_rest = self.cc_balance[self.cc_balance['YEAR_BALANCE'] >= 2].groupby('SK_ID_PREV').agg(aggregations_for_year)\n",
    "        cc_balance_aggregated_year_rest.columns = ['_'.join(ele).upper() + '_YEAR_REST' for ele in cc_balance_aggregated_year_rest.columns]\n",
    "        #merging all the yearwise aggregations\n",
    "        cc_balance_aggregated_year = cc_balance_aggregated_year.merge(cc_balance_aggregated_year_rest, on = 'SK_ID_PREV', how = 'outer')\n",
    "        self.cc_balance = self.cc_balance.drop('YEAR_BALANCE', axis = 1)\n",
    "\n",
    "        #merging all the aggregations\n",
    "        cc_aggregated = cc_balance_aggregated_overall.merge(cc_balance_aggregated_categories, on = 'SK_ID_PREV', how = 'outer')\n",
    "        cc_aggregated = cc_aggregated.merge(cc_balance_aggregated_year, on = 'SK_ID_PREV', how = 'outer')\n",
    "\n",
    "        #one-hot encoding the categorical column NAME_CONTRACT_STATUS\n",
    "        name_contract_dummies = pd.get_dummies(self.cc_balance.NAME_CONTRACT_STATUS, prefix='CONTRACT')\n",
    "        contract_names = name_contract_dummies.columns.tolist()     \n",
    "        #merging the one-hot encoded feature with original table\n",
    "        self.cc_balance = pd.concat([self.cc_balance, name_contract_dummies], axis=1)\n",
    "        #aggregating over SK_ID_PREV the one-hot encoded columns\n",
    "        aggregated_cc_contract = self.cc_balance[['SK_ID_PREV'] + contract_names].groupby('SK_ID_PREV').mean()\n",
    "\n",
    "        #merging with the aggregated table\n",
    "        cc_aggregated = cc_aggregated.merge(aggregated_cc_contract, on = 'SK_ID_PREV', how = 'outer')\n",
    "\n",
    "        #now we will aggregate on SK_ID_CURR\n",
    "        #As seen from EDA, since most of the SK_ID_CURR had only 1 credit card, so for aggregations, we will simply take the means\n",
    "        cc_aggregated = cc_aggregated.groupby('SK_ID_CURR', as_index = False).mean()\n",
    "                    \n",
    "        return cc_aggregated\n",
    "                    \n",
    "    def main(self):\n",
    "        '''\n",
    "        Function to be called for complete preprocessing and aggregation of credit_card_balance table.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            Final pre=processed and aggregated credit_card_balance table.\n",
    "        '''\n",
    "        \n",
    "        #loading the dataframe \n",
    "        self.load_dataframe()\n",
    "        #preprocessing and performing Feature Engineering\n",
    "        self.data_preprocessing_and_feature_engineering()\n",
    "        #aggregating over SK_ID_PREV and SK_ID_CURR\n",
    "        cc_aggregated = self.aggregations()\n",
    "\n",
    "        if self.verbose:\n",
    "            print('\\nDone preprocessing credit_card_balance.')\n",
    "            print(f\"\\nInitial Size of credit_card_balance: {self.initial_size}\")\n",
    "            print(f'Size of credit_card_balance after Pre-Processing, Feature Engineering and Aggregation: {cc_aggregated.shape}')\n",
    "            print(f'\\nTotal Time Taken = {datetime.now() - self.start}')\n",
    "\n",
    "        if self.dump_to_pickle:\n",
    "            if self.verbose:\n",
    "                print('\\nPickling pre-processed credit_card_balance to credit_card_balance_preprocessed.pkl')\n",
    "            with open(self.file_directory + 'credit_card_balance_preprocessed.pkl', 'wb') as f:\n",
    "                pickle.dump(cc_aggregated, f)\n",
    "            if self.verbose:\n",
    "                print('Done.')\n",
    "        if self.verbose:\n",
    "            print('-'*100)\n",
    "                    \n",
    "        return cc_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T08:51:59.861723Z",
     "start_time": "2020-10-23T08:38:07.653534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################\n",
      "#        Pre-processing credit_card_balance.csv         #\n",
      "#########################################################\n",
      "\n",
      "Loading the DataFrame, credit_card_balance.csv, into memory...\n",
      "Loaded credit_card_balance.csv\n",
      "Time Taken to load = 0:00:03.877409\n",
      "\n",
      "Starting Preprocessing and Feature Engineering...\n",
      "Done.\n",
      "Time Taken = 0:00:42.005148\n",
      "\n",
      "Aggregating the DataFrame, first over SK_ID_PREv, then over SK_ID_CURR\n",
      "\n",
      "Done preprocessing credit_card_balance.\n",
      "\n",
      "Initial Size of credit_card_balance: (3227965, 23)\n",
      "Size of credit_card_balance after Pre-Processing, Feature Engineering and Aggregation: (86905, 249)\n",
      "\n",
      "Total Time Taken = 0:00:52.687315\n",
      "\n",
      "Pickling pre-processed credit_card_balance to credit_card_balance_preprocessed.pkl\n",
      "Done.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cc_aggregated = preprocess_credit_card_balance(dump_to_pickle = True).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### application_train and application_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These tables consists of static data relating to the Borrowers. Each row represents one loan application.\n",
    "\n",
    "<ol><li>First we start with cleaning the data by removing the erroneous datapoints. We also remove the rows in train data with categories such that those categories do not appear in test data. We also convert the Region Rating features to categorical becuase we saw from the EDA that they don't follow an ordinal beviour when it comes to Defaulting Characteristics.</li>\n",
    "    <li>Inspired from the winner's writeup for the problem, we also predict the missing values of EXT_SOURCE features by building a regression model on the rest of the numeric features.</li>\n",
    "    <li>Next we do feature engineering on the numeric features, and generate features based on Domain Knoweldge, such as INCOME TO ANNUITY ratio, EXT_SOURCE means, etc.</li>\n",
    "    <li>We also try to predict the interest rates by using the data from the previous applications features, and predicting using the data from application_train features. We also create a feature based on the Target values from application_train where we compute the mean of targets of 500 nearest neighbors of each row.</li>\n",
    "    <li>Next we create some features based on the categorical interactions by grouping the data on several categorical combinations and imputing the aggregates for each group as features.</li>\n",
    "    <li>We encode the categorical features by response coding, as we didn't want to increase dimensionality by many-folds using OHE. </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T06:45:22.359701Z",
     "start_time": "2020-10-25T06:45:22.284867Z"
    }
   },
   "outputs": [],
   "source": [
    "class preprocess_application_train_test:\n",
    "    '''\n",
    "    Preprocess the application_train and application_test tables.\n",
    "    Contains 11 member functions:\n",
    "        1. init method\n",
    "        2. load_dataframe method\n",
    "        3. data_cleaning method\n",
    "        4. ext_source_values_predictor method\n",
    "        5. numeric_feature_engineering method\n",
    "        6. neighbors_EXT_SOURCE_feature method\n",
    "        7. categorical_interaction_features method\n",
    "        8. response_fit method\n",
    "        9. response_transform method\n",
    "        10. cnt_payment_prediction method\n",
    "        11. main method\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, file_directory = '', verbose = True, dump_to_pickle = False):\n",
    "        '''\n",
    "        This function is used to initialize the class members \n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            file_directory: Path, str, default = ''\n",
    "                The path where the file exists. Include a '/' at the end of the path in input\n",
    "            verbose: bool, default = True\n",
    "                Whether to enable verbosity or not\n",
    "            dump_to_pickle: bool, default = False\n",
    "                Whether to pickle the final preprocessed table or not\n",
    "                \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        self.dump_to_pickle = dump_to_pickle\n",
    "        self.file_directory = file_directory\n",
    "        \n",
    "    def load_dataframes(self):\n",
    "        '''\n",
    "        Function to load the application_train.csv and application_test.csv DataFrames.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "\n",
    "        if self.verbose:\n",
    "            self.start = datetime.now()\n",
    "            print('#######################################################')\n",
    "            print('#        Pre-processing application_train.csv         #')\n",
    "            print('#        Pre-processing application_test.csv          #')\n",
    "            print('#######################################################')\n",
    "            print(\"\\nLoading the DataFrame, credit_card_balance.csv, into memory...\")\n",
    "\n",
    "        self.application_train = pd.read_csv('dseb63_final_project_DP_dataset/dseb63_application_train.csv')\n",
    "        self.application_test = pd.read_csv('dseb63_final_project_DP_dataset/dseb63_application_test.csv')\n",
    "        self.initial_shape = self.application_train.shape\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Loaded application_train.csv and application_test.csv\")\n",
    "            print(f\"Time Taken to load = {datetime.now() - self.start}\")\n",
    "    \n",
    "    def data_cleaning(self):\n",
    "        '''\n",
    "        Function to clean the tables, by removing erroneous rows/entries.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\nPerforming Data Cleaning...\")\n",
    "        \n",
    "        #there are some FLAG_DOCUMENT features having just one category for almost all data, we will remove those\n",
    "        flag_cols_to_drop = ['FLAG_DOCUMENT_2','FLAG_DOCUMENT_4','FLAG_DOCUMENT_10','FLAG_DOCUMENT_12',\n",
    "                            'FLAG_DOCUMENT_20']\n",
    "        self.application_train = self.application_train.drop(flag_cols_to_drop, axis = 1)\n",
    "        self.application_test = self.application_test.drop(flag_cols_to_drop, axis = 1)\n",
    "        #converting age from days to years\n",
    "        self.application_train['DAYS_BIRTH'] = self.application_train['DAYS_BIRTH'] * -1 / 365\n",
    "        self.application_test['DAYS_BIRTH'] = self.application_test['DAYS_BIRTH'] * -1 / 365\n",
    "        #From the EDA we saw some erroneous values in DAYS_EMPLOYED field \n",
    "        self.application_train['DAYS_EMPLOYED'][self.application_train['DAYS_EMPLOYED'] == 365243] = np.nan\n",
    "        self.application_test['DAYS_EMPLOYED'][self.application_test['DAYS_EMPLOYED'] == 365243] = np.nan\n",
    "        #OBS Columns have an erroneous value, we'll remove those values \n",
    "        self.application_train['OBS_30_CNT_SOCIAL_CIRCLE'][self.application_train['OBS_30_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "        self.application_train['OBS_60_CNT_SOCIAL_CIRCLE'][self.application_train['OBS_60_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "        self.application_test['OBS_30_CNT_SOCIAL_CIRCLE'][self.application_test['OBS_30_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "        self.application_test['OBS_60_CNT_SOCIAL_CIRCLE'][self.application_test['OBS_60_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "        #there were also 4 rows with 'XNA' as Gender, removing these rows\n",
    "        self.application_train = self.application_train[self.application_train['CODE_GENDER'] != 'XNA']\n",
    "        #filling the categorical columns with 'XNA' value\n",
    "        categorical_columns = self.application_train.dtypes[self.application_train.dtypes == 'object'].index.tolist()\n",
    "        self.application_train[categorical_columns] = self.application_train[categorical_columns].fillna('XNA')\n",
    "        self.application_test[categorical_columns] = self.application_test[categorical_columns].fillna('XNA')\n",
    "        #converting columns of REGION_RATING_CLIENT to object type, as we saw some complex impact on TARGET variable during EDA \n",
    "        self.application_train['REGION_RATING_CLIENT'] = self.application_train['REGION_RATING_CLIENT'].astype('object')\n",
    "        self.application_train['REGION_RATING_CLIENT_W_CITY'] = self.application_train['REGION_RATING_CLIENT_W_CITY'].astype('object')\n",
    "        self.application_test['REGION_RATING_CLIENT'] = self.application_test['REGION_RATING_CLIENT'].astype('object')\n",
    "        self.application_test['REGION_RATING_CLIENT_W_CITY'] = self.application_test['REGION_RATING_CLIENT_W_CITY'].astype('object')\n",
    "        #counting the total NaN values for each application\n",
    "        self.application_train['MISSING_VALS_TOTAL_APP'] = self.application_train.isna().sum(axis = 1)\n",
    "        self.application_test['MISSING_VALS_TOTAL_APP'] = self.application_test.isna().sum(axis = 1)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"Done.\")\n",
    "            \n",
    "    def ext_source_values_predictor(self):\n",
    "        '''\n",
    "        Function to predict the missing values of EXT_SOURCE features\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            start = datetime.now()\n",
    "            print(\"\\nPredicting the missing values of EXT_SOURCE columns...\")\n",
    "            \n",
    "        #predicting the EXT_SOURCE missing values\n",
    "        #using only numeric columns for predicting the EXT_SOURCES\n",
    "        columns_for_modelling = list(set(self.application_test.dtypes[self.application_test.dtypes != 'object'].index.tolist())\n",
    "                                             - set(['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','SK_ID_CURR']))\n",
    "        with open('columns_for_ext_values_predictor.pkl', 'wb') as f:\n",
    "            pickle.dump(columns_for_modelling, f)\n",
    "        \n",
    "        #we'll train an XGB Regression model for predicting missing EXT_SOURCE values\n",
    "        #we will predict in the order of least number of missing value columns to max.\n",
    "        for ext_col in ['EXT_SOURCE_2','EXT_SOURCE_3','EXT_SOURCE_1']:\n",
    "            #X_model - datapoints which do not have missing values of given column\n",
    "            #Y_train - values of column trying to predict with non missing values\n",
    "            #X_train_missing - datapoints in application_train with missing values\n",
    "            #X_test_missing - datapoints in application_test with missing values\n",
    "            X_model, X_train_missing, X_test_missing, Y_train = self.application_train[~self.application_train[ext_col].isna()][columns_for_modelling], self.application_train[\n",
    "                                                                self.application_train[ext_col].isna()][columns_for_modelling], self.application_test[\n",
    "                                                                self.application_test[ext_col].isna()][columns_for_modelling], self.application_train[\n",
    "                                                                ext_col][~self.application_train[ext_col].isna()]\n",
    "            xg = XGBRegressor(n_estimators = 1000, max_depth = 3, learning_rate = 0.1, n_jobs = -1, random_state = 59)\n",
    "            xg.fit(X_model, Y_train)\n",
    "            #dumping the model to pickle file\n",
    "            with open(f'nan_{ext_col}_xgbr_model.pkl', 'wb') as f:\n",
    "                pickle.dump(xg, f)\n",
    "\n",
    "            self.application_train[ext_col][self.application_train[ext_col].isna()] = xg.predict(X_train_missing)\n",
    "            self.application_test[ext_col][self.application_test[ext_col].isna()] = xg.predict(X_test_missing)\n",
    "            \n",
    "            #adding the predicted column to columns for modelling for next column's prediction\n",
    "            columns_for_modelling = columns_for_modelling + [ext_col]\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(\"Done.\")\n",
    "            print(f\"Time elapsed = {datetime.now() - start}\")\n",
    "                    \n",
    "    def numeric_feature_engineering(self, data):\n",
    "        '''\n",
    "        Function to perform feature engineering on numeric columns based on domain knowledge.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            data: DataFrame\n",
    "                The tables of whose features are to be generated\n",
    "        \n",
    "        Returns: \n",
    "            None\n",
    "        '''\n",
    "    \n",
    "        #income and credit features\n",
    "        data['CREDIT_INCOME_RATIO'] = data['AMT_CREDIT'] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "        data['CREDIT_ANNUITY_RATIO'] = data['AMT_CREDIT'] / (data['AMT_ANNUITY'] + 0.00001)\n",
    "        data['ANNUITY_INCOME_RATIO'] = data['AMT_ANNUITY'] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "        data['INCOME_ANNUITY_DIFF'] = data['AMT_INCOME_TOTAL'] - data['AMT_ANNUITY']\n",
    "        data['CREDIT_GOODS_RATIO'] = data['AMT_CREDIT'] / (data['AMT_GOODS_PRICE'] + 0.00001)\n",
    "        data['CREDIT_GOODS_DIFF'] = data['AMT_CREDIT'] - data['AMT_GOODS_PRICE'] + 0.00001\n",
    "        data['GOODS_INCOME_RATIO'] = data['AMT_GOODS_PRICE'] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "        data['INCOME_EXT_RATIO'] = data['AMT_INCOME_TOTAL'] / (data['EXT_SOURCE_3'] + 0.00001)\n",
    "        data['CREDIT_EXT_RATIO'] = data['AMT_CREDIT'] / (data['EXT_SOURCE_3'] + 0.00001)\n",
    "        #age ratios and diffs\n",
    "        data['AGE_EMPLOYED_DIFF'] = data['DAYS_BIRTH'] - data['DAYS_EMPLOYED']\n",
    "        data['EMPLOYED_TO_AGE_RATIO'] = data['DAYS_EMPLOYED'] / (data['DAYS_BIRTH'] + 0.00001)\n",
    "        #car ratios\n",
    "        data['CAR_EMPLOYED_DIFF'] = data['OWN_CAR_AGE'] - data['DAYS_EMPLOYED']\n",
    "        data['CAR_EMPLOYED_RATIO'] = data['OWN_CAR_AGE'] / (data['DAYS_EMPLOYED']+0.00001)\n",
    "        data['CAR_AGE_DIFF'] = data['DAYS_BIRTH'] - data['OWN_CAR_AGE']\n",
    "        data['CAR_AGE_RATIO'] = data['OWN_CAR_AGE'] / (data['DAYS_BIRTH'] + 0.00001)\n",
    "        #flag contacts sum\n",
    "        data['FLAG_CONTACTS_SUM'] = data['FLAG_MOBIL'] + data['FLAG_EMP_PHONE'] + data['FLAG_WORK_PHONE'] + data[\n",
    "                                    'FLAG_CONT_MOBILE'] + data['FLAG_PHONE'] + data['FLAG_EMAIL']\n",
    "        \n",
    "        data['HOUR_PROCESS_CREDIT_MUL'] = data['AMT_CREDIT'] * data['HOUR_APPR_PROCESS_START']\n",
    "        #family members\n",
    "        data['CNT_NON_CHILDREN'] = data['CNT_FAM_MEMBERS'] - data['CNT_CHILDREN']\n",
    "        data['CHILDREN_INCOME_RATIO'] = data['CNT_CHILDREN'] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "        data['PER_CAPITA_INCOME'] = data['AMT_INCOME_TOTAL'] / (data['CNT_FAM_MEMBERS'] + 1)\n",
    "        #region ratings\n",
    "        data['REGIONS_RATING_INCOME_MUL'] = (data['REGION_RATING_CLIENT'] + data['REGION_RATING_CLIENT_W_CITY']) * data['AMT_INCOME_TOTAL'] / 2\n",
    "        data['REGION_RATING_MAX'] = [max(ele1, ele2) for ele1, ele2 in zip(data['REGION_RATING_CLIENT'], data['REGION_RATING_CLIENT_W_CITY'])]\n",
    "        data['REGION_RATING_MAX'] = [min(ele1, ele2) for ele1, ele2 in zip(data['REGION_RATING_CLIENT'], data['REGION_RATING_CLIENT_W_CITY'])]\n",
    "        data['REGION_RATING_MEAN'] = (data['REGION_RATING_CLIENT'] + data['REGION_RATING_CLIENT_W_CITY']) / 2\n",
    "        data['REGION_RATING_MUL'] = data['REGION_RATING_CLIENT'] * data['REGION_RATING_CLIENT_W_CITY']\n",
    "        #flag regions\n",
    "        data['FLAG_REGIONS'] = data['REG_REGION_NOT_LIVE_REGION'] + data['REG_REGION_NOT_WORK_REGION'] + data['LIVE_REGION_NOT_WORK_REGION']+data[\n",
    "                                'REG_CITY_NOT_LIVE_CITY'] + data['REG_CITY_NOT_WORK_CITY'] + data['LIVE_CITY_NOT_WORK_CITY']   \n",
    "        #ext_sources\n",
    "        data['EXT_SOURCE_MEAN'] = (data['EXT_SOURCE_1'] + data['EXT_SOURCE_2'] + data['EXT_SOURCE_3'] ) / 3\n",
    "        data['EXT_SOURCE_MUL'] = data['EXT_SOURCE_1'] * data['EXT_SOURCE_2'] * data['EXT_SOURCE_3'] \n",
    "        data['EXT_SOURCE_MAX'] = [max(ele1,ele2,ele3) for ele1, ele2, ele3 in zip(data['EXT_SOURCE_1'], data['EXT_SOURCE_2'], data['EXT_SOURCE_3'])]\n",
    "        data['EXT_SOURCE_MIN'] = [min(ele1,ele2,ele3) for ele1, ele2, ele3 in zip(data['EXT_SOURCE_1'], data['EXT_SOURCE_2'], data['EXT_SOURCE_3'])]\n",
    "        data['EXT_SOURCE_VAR'] = [np.var([ele1,ele2,ele3]) for ele1, ele2, ele3 in zip(data['EXT_SOURCE_1'], data['EXT_SOURCE_2'], data['EXT_SOURCE_3'])]\n",
    "        data['WEIGHTED_EXT_SOURCE'] =  data.EXT_SOURCE_1 * 2 + data.EXT_SOURCE_2 * 3 + data.EXT_SOURCE_3 * 4\n",
    "        #apartment scores\n",
    "        data['APARTMENTS_SUM_AVG'] = data['APARTMENTS_AVG'] + data['BASEMENTAREA_AVG'] + data['YEARS_BEGINEXPLUATATION_AVG'] + data[\n",
    "                                    'YEARS_BUILD_AVG'] + data['COMMONAREA_AVG'] + data['ELEVATORS_AVG'] + data['ENTRANCES_AVG'] + data[\n",
    "                                    'FLOORSMAX_AVG'] + data['FLOORSMIN_AVG'] + data['LANDAREA_AVG'] + data['LIVINGAPARTMENTS_AVG'] + data[\n",
    "                                    'LIVINGAREA_AVG'] + data['NONLIVINGAPARTMENTS_AVG'] + data['NONLIVINGAREA_AVG']\n",
    "\n",
    "        data['APARTMENTS_SUM_MODE'] = data['APARTMENTS_MODE'] + data['BASEMENTAREA_MODE'] + data['YEARS_BEGINEXPLUATATION_MODE'] + data[\n",
    "                                    'YEARS_BUILD_MODE'] + data['COMMONAREA_MODE'] + data['ELEVATORS_MODE'] + data['ENTRANCES_MODE'] + data[\n",
    "                                    'FLOORSMAX_MODE'] + data['FLOORSMIN_MODE'] + data['LANDAREA_MODE'] + data['LIVINGAPARTMENTS_MODE'] + data[\n",
    "                                    'LIVINGAREA_MODE'] + data['NONLIVINGAPARTMENTS_MODE'] + data['NONLIVINGAREA_MODE'] + data['TOTALAREA_MODE']\n",
    "\n",
    "        data['APARTMENTS_SUM_MEDI'] = data['APARTMENTS_MEDI'] + data['BASEMENTAREA_MEDI'] + data['YEARS_BEGINEXPLUATATION_MEDI'] + data[\n",
    "                                    'YEARS_BUILD_MEDI'] + data['COMMONAREA_MEDI'] + data['ELEVATORS_MEDI'] + data['ENTRANCES_MEDI'] + data[\n",
    "                                    'FLOORSMAX_MEDI'] + data['FLOORSMIN_MEDI'] + data['LANDAREA_MEDI'] + data['LIVINGAPARTMENTS_MEDI'] + data[\n",
    "                                    'LIVINGAREA_MEDI'] + data['NONLIVINGAPARTMENTS_MEDI'] + data['NONLIVINGAREA_MEDI']\n",
    "        data['INCOME_APARTMENT_AVG_MUL'] = data['APARTMENTS_SUM_AVG'] * data['AMT_INCOME_TOTAL']\n",
    "        data['INCOME_APARTMENT_MODE_MUL'] = data['APARTMENTS_SUM_MODE'] * data['AMT_INCOME_TOTAL']\n",
    "        data['INCOME_APARTMENT_MEDI_MUL'] = data['APARTMENTS_SUM_MEDI'] * data['AMT_INCOME_TOTAL']\n",
    "        #OBS And DEF\n",
    "        data['OBS_30_60_SUM'] = data['OBS_30_CNT_SOCIAL_CIRCLE'] + data['OBS_60_CNT_SOCIAL_CIRCLE']\n",
    "        data['DEF_30_60_SUM'] = data['DEF_30_CNT_SOCIAL_CIRCLE'] + data['DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "        data['OBS_DEF_30_MUL'] = data['OBS_30_CNT_SOCIAL_CIRCLE'] *  data['DEF_30_CNT_SOCIAL_CIRCLE']\n",
    "        data['OBS_DEF_60_MUL'] = data['OBS_60_CNT_SOCIAL_CIRCLE'] *  data['DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "        data['SUM_OBS_DEF_ALL'] = data['OBS_30_CNT_SOCIAL_CIRCLE'] + data['DEF_30_CNT_SOCIAL_CIRCLE'] + data[\n",
    "                                    'OBS_60_CNT_SOCIAL_CIRCLE'] + data['DEF_60_CNT_SOCIAL_CIRCLE']\n",
    "        data['OBS_30_CREDIT_RATIO'] = data['AMT_CREDIT'] / (data['OBS_30_CNT_SOCIAL_CIRCLE'] + 0.00001)\n",
    "        data['OBS_60_CREDIT_RATIO'] = data['AMT_CREDIT'] / (data['OBS_60_CNT_SOCIAL_CIRCLE'] + 0.00001)\n",
    "        data['DEF_30_CREDIT_RATIO'] = data['AMT_CREDIT'] / (data['DEF_30_CNT_SOCIAL_CIRCLE'] + 0.00001)\n",
    "        data['DEF_60_CREDIT_RATIO'] = data['AMT_CREDIT'] / (data['DEF_60_CNT_SOCIAL_CIRCLE'] + 0.00001)\n",
    "        #Flag Documents combined\n",
    "        data['SUM_FLAGS_DOCUMENTS'] = data['FLAG_DOCUMENT_3'] + data['FLAG_DOCUMENT_5'] + data['FLAG_DOCUMENT_6']  + data[\n",
    "                                    'FLAG_DOCUMENT_7'] + data['FLAG_DOCUMENT_8'] + data['FLAG_DOCUMENT_9'] + data[\n",
    "                                    'FLAG_DOCUMENT_11'] + data['FLAG_DOCUMENT_13'] + data['FLAG_DOCUMENT_14'] + data[\n",
    "                                    'FLAG_DOCUMENT_15'] + data['FLAG_DOCUMENT_16'] + data['FLAG_DOCUMENT_17'] + data[\n",
    "                                    'FLAG_DOCUMENT_18'] + data['FLAG_DOCUMENT_19'] + data['FLAG_DOCUMENT_21']\n",
    "        #details change\n",
    "        data['DAYS_DETAILS_CHANGE_MUL'] = data['DAYS_LAST_PHONE_CHANGE'] * data['DAYS_REGISTRATION'] * data['DAYS_ID_PUBLISH']\n",
    "        data['DAYS_DETAILS_CHANGE_SUM'] = data['DAYS_LAST_PHONE_CHANGE'] + data['DAYS_REGISTRATION'] + data['DAYS_ID_PUBLISH']\n",
    "        #enquires\n",
    "        data['AMT_ENQ_SUM'] = data['AMT_REQ_CREDIT_BUREAU_HOUR'] + data['AMT_REQ_CREDIT_BUREAU_DAY'] + data['AMT_REQ_CREDIT_BUREAU_WEEK'] + data[\n",
    "                            'AMT_REQ_CREDIT_BUREAU_MON'] + data['AMT_REQ_CREDIT_BUREAU_QRT'] + data['AMT_REQ_CREDIT_BUREAU_YEAR']\n",
    "        data['ENQ_CREDIT_RATIO'] = data['AMT_ENQ_SUM'] / (data['AMT_CREDIT'] + 0.00001)\n",
    "        \n",
    "        cnt_payment = self.cnt_payment_prediction(data)\n",
    "        data['EXPECTED_CNT_PAYMENT'] = cnt_payment\n",
    "        data['EXPECTED_INTEREST'] = data['AMT_ANNUITY'] *  data['EXPECTED_CNT_PAYMENT'] - data['AMT_CREDIT']\n",
    "        data['EXPECTED_INTEREST_SHARE'] = data['EXPECTED_INTEREST'] / (data['AMT_CREDIT'] + 0.00001)\n",
    "        data['EXPECTED_INTEREST_RATE'] = 2 * 12 * data['EXPECTED_INTEREST'] / (data['AMT_CREDIT'] * (data['EXPECTED_CNT_PAYMENT'] + 1))\n",
    "                    \n",
    "        return data\n",
    "    \n",
    "    def neighbors_EXT_SOURCE_feature(self):\n",
    "        '''\n",
    "        Function to generate a feature which contains the means of TARGET of 500 neighbors of a particular row.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        #https://www.kaggle.com/c/home-credit-default-risk/discussion/64821\n",
    "        #imputing the mean of 500 nearest neighbor's target values for each application\n",
    "        #neighbors are computed using EXT_SOURCE feature and CREDIT_ANNUITY_RATIO\n",
    "        \n",
    "        knn = KNeighborsClassifier(500, n_jobs = -1)\n",
    "        \n",
    "        train_data_for_neighbors = self.application_train[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','CREDIT_ANNUITY_RATIO']].fillna(0)\n",
    "        #saving the training data for neighbors\n",
    "        with open('TARGET_MEAN_500_Neighbors_training_data.pkl', 'wb') as f:\n",
    "            pickle.dump(train_data_for_neighbors, f)\n",
    "        train_target = self.application_train.TARGET\n",
    "        test_data_for_neighbors = self.application_test[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3','CREDIT_ANNUITY_RATIO']].fillna(0)\n",
    "        \n",
    "        knn.fit(train_data_for_neighbors, train_target)\n",
    "        #pickling the knn model\n",
    "        with open('KNN_model_TARGET_500_neighbors.pkl', 'wb') as f:\n",
    "            pickle.dump(knn, f)\n",
    "            \n",
    "        train_500_neighbors = knn.kneighbors(train_data_for_neighbors)[1]\n",
    "        test_500_neighbors = knn.kneighbors(test_data_for_neighbors)[1]\n",
    "        \n",
    "        #adding the means of targets of 500 neighbors to new column\n",
    "        self.application_train['TARGET_NEIGHBORS_500_MEAN'] = [self.application_train['TARGET'].iloc[ele].mean() for ele in train_500_neighbors]\n",
    "        self.application_test['TARGET_NEIGHBORS_500_MEAN'] = [self.application_train['TARGET'].iloc[ele].mean() for ele in test_500_neighbors]\n",
    "    \n",
    "    def categorical_interaction_features(self, train_data, test_data):\n",
    "        '''\n",
    "        Function to generate some features based on categorical groupings.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            train_data, test_data : DataFrames\n",
    "                train and test dataframes\n",
    "        \n",
    "        Returns:\n",
    "            Train and test datasets, with added categorical interaction features.\n",
    "        '''\n",
    "        \n",
    "        #now we will create features based on categorical interactions\n",
    "        columns_to_aggregate_on = [\n",
    "            ['NAME_CONTRACT_TYPE', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE'],\n",
    "            ['CODE_GENDER', 'NAME_FAMILY_STATUS', 'NAME_INCOME_TYPE'],\n",
    "            ['FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'NAME_INCOME_TYPE'],\n",
    "            ['NAME_EDUCATION_TYPE','NAME_INCOME_TYPE','OCCUPATION_TYPE'],\n",
    "            ['OCCUPATION_TYPE','ORGANIZATION_TYPE'],\n",
    "            ['CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY']\n",
    "\n",
    "        ]\n",
    "        aggregations = {\n",
    "            'AMT_ANNUITY' : ['mean','max','min'],\n",
    "            'ANNUITY_INCOME_RATIO' : ['mean','max','min'],\n",
    "            'AGE_EMPLOYED_DIFF' : ['mean','min'],\n",
    "            'AMT_INCOME_TOTAL' : ['mean','max','min'],\n",
    "            'APARTMENTS_SUM_AVG' : ['mean','max','min'],\n",
    "            'APARTMENTS_SUM_MEDI' : ['mean','max','min'],\n",
    "            'EXT_SOURCE_MEAN' : ['mean','max','min'],\n",
    "            'EXT_SOURCE_1' : ['mean','max','min'],\n",
    "            'EXT_SOURCE_2' : ['mean','max','min'],\n",
    "            'EXT_SOURCE_3' : ['mean','max','min']\n",
    "        }\n",
    "        \n",
    "        #extracting values\n",
    "        for group in columns_to_aggregate_on:\n",
    "            #grouping based on categories\n",
    "            grouped_interactions = train_data.groupby(group).agg(aggregations) \n",
    "            grouped_interactions.columns = ['_'.join(ele).upper() + '_AGG_' + '_'.join(group) for ele in grouped_interactions.columns]\n",
    "            #saving the grouped interactions to pickle file\n",
    "            group_name = '_'.join(group)\n",
    "            with open(f'Application_train_grouped_interactions_{group_name}.pkl', 'wb') as f:\n",
    "                pickle.dump(grouped_interactions, f)\n",
    "            #merging with the original data\n",
    "            train_data = train_data.join(grouped_interactions, on = group)\n",
    "            test_data = test_data.join(grouped_interactions, on = group)\n",
    "\n",
    "        return train_data, test_data\n",
    "\n",
    "    def response_fit(self, data, column):\n",
    "        '''\n",
    "        Response Encoding Fit Function\n",
    "        Function to create a vocabulary with the probability of occurrence of each category for categorical features\n",
    "        for a given class label.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            data: DataFrame\n",
    "                training Dataset\n",
    "            column: str\n",
    "                the categorical column for which vocab is to be generated\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of probability of occurrence of each category in a particular class label.\n",
    "        '''\n",
    "        \n",
    "        dict_occurrences = {1: {}, 0: {}}\n",
    "        for label in [0,1]:\n",
    "            dict_occurrences[label] = dict((data[column][data.TARGET == label].value_counts() / data[column].value_counts()).fillna(0))\n",
    "\n",
    "        return dict_occurrences\n",
    "\n",
    "    def response_transform(self, data, column, dict_mapping):\n",
    "        '''\n",
    "        Response Encoding Transform Function\n",
    "        Function to transform the categorical feature into two features, which contain the probability\n",
    "        of occurrence of that category for each class label.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            data: DataFrame\n",
    "                DataFrame whose categorical features are to be encoded\n",
    "            column: str\n",
    "                categorical column whose encoding is to be done\n",
    "            dict_mapping: dict\n",
    "                Dictionary obtained from Response Fit function for that particular column\n",
    "        \n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        data[column + '_0'] = data[column].map(dict_mapping[0])\n",
    "        data[column + '_1'] = data[column].map(dict_mapping[1])\n",
    "    \n",
    "    def cnt_payment_prediction(self, data_to_predict):\n",
    "        '''\n",
    "        Function to predict the Count_payments on Current Loans using data from previous loans.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            data_to_predict: DataFrame\n",
    "                the values using which the model would predict the Count_payments on current applications\n",
    "        \n",
    "        Returns:\n",
    "            Predicted Count_payments of the current applications.\n",
    "        '''\n",
    "        \n",
    "        #https://www.kaggle.com/c/home-credit-default-risk/discussion/64598\n",
    "        previous_application = pd.read_csv('dseb63_final_project_DP_dataset/dseb63_previous_application.csv')\n",
    "        train_data = previous_application[['AMT_CREDIT', 'AMT_ANNUITY', 'CNT_PAYMENT']].dropna()\n",
    "        train_data['CREDIT_ANNUITY_RATIO'] = train_data['AMT_CREDIT'] / (train_data['AMT_ANNUITY'] + 1)\n",
    "        #value to predict is our CNT_PAYMENT\n",
    "        train_value = train_data.pop('CNT_PAYMENT')\n",
    "        \n",
    "        #test data would be our application_train data\n",
    "        test_data = data_to_predict[['AMT_CREDIT','AMT_ANNUITY']].fillna(0)\n",
    "        test_data['CREDIT_ANNUITY_RATIO'] = test_data['AMT_CREDIT'] / (test_data['AMT_ANNUITY'] + 1)\n",
    "        \n",
    "        lgbmr = LGBMRegressor(max_depth = 9, n_estimators = 5000, n_jobs = -1, learning_rate = 0.3, \n",
    "                              random_state = 125)\n",
    "        lgbmr.fit(train_data, train_value)\n",
    "        #dumping the model to pickle file\n",
    "        with open('cnt_payment_predictor_lgbmr.pkl', 'wb') as f:\n",
    "            pickle.dump(lgbmr, f)\n",
    "        #predicting the CNT_PAYMENT for test_data\n",
    "        cnt_payment = lgbmr.predict(test_data)\n",
    "        \n",
    "        return cnt_payment\n",
    "\n",
    "    def main(self):\n",
    "        '''\n",
    "        Function to be called for complete preprocessing of application_train and application_test tables.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            \n",
    "        Returns:\n",
    "            Final pre=processed application_train and application_test tables.\n",
    "        '''\n",
    "\n",
    "        #loading the DataFrames first\n",
    "        self.load_dataframes()\n",
    "        #first doing Data Cleaning\n",
    "        self.data_cleaning()\n",
    "        #predicting the missing values of EXT_SOURCE columns\n",
    "        self.ext_source_values_predictor()\n",
    "        \n",
    "        #doing the feature engineering\n",
    "        if self.verbose:\n",
    "            start = datetime.now()\n",
    "            print(\"\\nStarting Feature Engineering...\")\n",
    "            print(\"\\nCreating Domain Based Features on Numeric Data\")\n",
    "        #Creating Numeric features based on domain knowledge\n",
    "        self.application_train = self.numeric_feature_engineering(self.application_train)\n",
    "        self.application_test = self.numeric_feature_engineering(self.application_test)\n",
    "        #500 Neighbors Target mean\n",
    "        self.neighbors_EXT_SOURCE_feature()\n",
    "        if self.verbose:\n",
    "            print(\"Done.\")\n",
    "            print(f\"Time Taken = {datetime.now() - start}\")\n",
    "\n",
    "        if self.verbose:\n",
    "            start = datetime.now()\n",
    "            print(\"Creating features based on Categorical Interactions on some Numeric Features\")\n",
    "        #creating features based on categorical interactions\n",
    "        self.application_train, self.application_test = self.categorical_interaction_features(self.application_train, self.application_test)   \n",
    "        if self.verbose:\n",
    "            print(\"Done.\")\n",
    "            print(f\"Time taken = {datetime.now() - start}\")\n",
    "        \n",
    "        #using response coding on categorical features, to keep the dimensionality in check\n",
    "        #categorical columns to perform response coding on\n",
    "        categorical_columns_application = self.application_train.dtypes[self.application_train.dtypes == 'object'].index.tolist()\n",
    "        for col in categorical_columns_application:\n",
    "            #extracting the dictionary with values corresponding to TARGET variable 0 and 1 for each of the categories\n",
    "            mapping_dictionary = self.response_fit(self.application_train, col)\n",
    "            #saving the mapping dictionary to pickle file\n",
    "            with open(f'Response_coding_dict_{col}.pkl', 'wb') as f:\n",
    "                pickle.dump(mapping_dictionary, f)\n",
    "            #mapping this dictionary with our DataFrame\n",
    "            self.response_transform(self.application_train, col, mapping_dictionary)\n",
    "            self.response_transform(self.application_test, col, mapping_dictionary)\n",
    "            #removing the original categorical columns\n",
    "            _ = self.application_train.pop(col)\n",
    "            _ = self.application_test.pop(col)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('Done preprocessing appplication_train and application_test.')\n",
    "            print(f\"\\nInitial Size of application_train: {self.initial_shape}\")\n",
    "            print(f'Size of application_train after Pre-Processing and Feature Engineering: {self.application_train.shape}')\n",
    "            print(f'\\nTotal Time Taken = {datetime.now() - self.start}')\n",
    "\n",
    "        if self.dump_to_pickle:\n",
    "            if self.verbose:\n",
    "                print('\\nPickling pre-processed application_train and application_test to application_train_preprocessed.pkl and application_test_preprocessed, respectively.')\n",
    "            with open(self.file_directory + 'application_train_preprocessed.pkl', 'wb') as f:\n",
    "                pickle.dump(self.application_train, f)\n",
    "            with open(self.file_directory + 'application_test_preprocessed.pkl', 'wb') as f:\n",
    "                pickle.dump(self.application_test, f)\n",
    "            if self.verbose:\n",
    "                print('Done.')  \n",
    "        if self.verbose:\n",
    "            print('-'*100)\n",
    "                        \n",
    "        return self.application_train, self.application_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T06:57:14.009334Z",
     "start_time": "2020-10-25T06:45:23.062899Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "#        Pre-processing application_train.csv         #\n",
      "#        Pre-processing application_test.csv          #\n",
      "#######################################################\n",
      "\n",
      "Loading the DataFrame, credit_card_balance.csv, into memory...\n",
      "Loaded application_train.csv and application_test.csv\n",
      "Time Taken to load = 0:00:02.242037\n",
      "\n",
      "Performing Data Cleaning...\n",
      "Done.\n",
      "\n",
      "Predicting the missing values of EXT_SOURCE columns...\n",
      "Done.\n",
      "Time elapsed = 0:00:34.218974\n",
      "\n",
      "Starting Feature Engineering...\n",
      "\n",
      "Creating Domain Based Features on Numeric Data\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 1106482, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 16.051980\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004283 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 765\n",
      "[LightGBM] [Info] Number of data points in the train set: 1106482, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 16.051980\n",
      "Done.\n",
      "Time Taken = 0:02:50.225854\n",
      "Creating features based on Categorical Interactions on some Numeric Features\n",
      "Done.\n",
      "Time taken = 0:00:02.488911\n",
      "Done preprocessing appplication_train and application_test.\n",
      "\n",
      "Initial Size of application_train: (246009, 123)\n",
      "Size of application_train after Pre-Processing and Feature Engineering: (246006, 370)\n",
      "\n",
      "Total Time Taken = 0:03:31.769104\n",
      "\n",
      "Pickling pre-processed application_train and application_test to application_train_preprocessed.pkl and application_test_preprocessed, respectively.\n",
      "Done.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "application_train, application_test = preprocess_application_train_test(dump_to_pickle = True).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging All Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will merge all the preprocessed tables with the application_train and application_test tables. The merges will be Left Outer Joins, such that all the current applications are preserved, as we have to model on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T20:02:02.573475Z",
     "start_time": "2020-10-24T20:02:02.564500Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_all_tables(application_train, application_test, bureau_aggregated, previous_aggregated, \n",
    "                    installments_aggregated, pos_aggregated, cc_aggregated):\n",
    "    '''\n",
    "    Function to merge all the tables together with the application_train and application_test tables\n",
    "    on SK_ID_CURR.\n",
    "    \n",
    "    Inputs:\n",
    "        All the previously pre-processed Tables.\n",
    "        \n",
    "    Returns:\n",
    "        Single merged tables, one for training data and one for test data\n",
    "    '''\n",
    "\n",
    "    #merging application_train and application_test with Aggregated bureau table\n",
    "    app_train_merged = application_train.merge(bureau_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
    "    app_test_merged = application_test.merge(bureau_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
    "    #merging with aggregated previous_applications\n",
    "    app_train_merged = app_train_merged.merge(previous_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
    "    app_test_merged = app_test_merged.merge(previous_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
    "    #merging with aggregated installments tables\n",
    "    app_train_merged = app_train_merged.merge(installments_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
    "    app_test_merged = app_test_merged.merge(installments_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
    "    #merging with aggregated POS_Cash balance table\n",
    "    app_train_merged = app_train_merged.merge(pos_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
    "    app_test_merged = app_test_merged.merge(pos_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
    "    #merging with aggregated credit card table\n",
    "    app_train_merged = app_train_merged.merge(cc_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
    "    app_test_merged = app_test_merged.merge(cc_aggregated, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "    return reduce_mem_usage(app_train_merged), reduce_mem_usage(app_test_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T09:06:40.873978Z",
     "start_time": "2020-10-23T09:04:28.790785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Memory usage of dataframe: 2905.41 MB\n",
      "Memory usage after optimization: 1018.91 MB\n",
      "Decreased by 64.9%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Memory usage of dataframe: 725.89 MB\n",
      "Memory usage after optimization: 252.91 MB\n",
      "Decreased by 65.2%\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = merge_all_tables(application_train, application_test, \n",
    "                                         bureau_aggregated, previous_aggregated, \n",
    "                                         installments_aggregated, pos_aggregated, \n",
    "                                         cc_aggregated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Based on Interaction Among Different Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create some more features based on interactions between different tables. For example, we will calculate the Annuity to income ratio for previous applications, similarly we will calculate Credit to income ratios, and several such features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T16:04:25.854127Z",
     "start_time": "2020-10-24T16:04:25.837198Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_new_features(data):\n",
    "    '''\n",
    "    Function to create few more features after the merging of features, by using the\n",
    "    interactions between various tables.\n",
    "    \n",
    "    Inputs:\n",
    "        data: DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    #previous applications columns\n",
    "    prev_annuity_columns = [ele for ele in previous_aggregated.columns if 'AMT_ANNUITY' in ele]\n",
    "    for col in prev_annuity_columns:\n",
    "        data['PREV_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    prev_goods_columns = [ele for ele in previous_aggregated.columns if 'AMT_GOODS' in ele]\n",
    "    for col in prev_goods_columns:\n",
    "        data['PREV_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "  \n",
    "    #credit_card_balance columns\n",
    "    cc_amt_principal_cols = [ele for ele in cc_aggregated.columns if 'AMT_RECEIVABLE_PRINCIPAL' in ele]\n",
    "    for col in cc_amt_principal_cols:\n",
    "        data['CC_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    cc_amt_recivable_cols = [ele for ele in cc_aggregated.columns if 'AMT_RECIVABLE' in ele]\n",
    "    for col in cc_amt_recivable_cols:\n",
    "        data['CC_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    cc_amt_total_receivable_cols = [ele for ele in cc_aggregated.columns if 'TOTAL_RECEIVABLE' in ele]\n",
    "    for col in cc_amt_total_receivable_cols:\n",
    "        data['CC_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    \n",
    "    #installments_payments columns\n",
    "    installments_payment_cols = [ele for ele in installments_aggregated.columns if 'AMT_PAYMENT' in ele and 'RATIO' not in ele and 'DIFF' not in ele]\n",
    "    for col in installments_payment_cols:\n",
    "        data['INSTALLMENTS_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    #https://www.kaggle.com/c/home-credit-default-risk/discussion/64821\n",
    "    installments_max_installment = ['AMT_INSTALMENT_MEAN_MAX', 'AMT_INSTALMENT_SUM_MAX']\n",
    "    for col in installments_max_installment:\n",
    "        data['INSTALLMENTS_ANNUITY_' + col + '_RATIO'] = data['AMT_ANNUITY'] / (data[col] + 0.00001)\n",
    "    \n",
    "    #POS_CASH_balance features have been created in its own dataframe itself\n",
    "\n",
    "    #bureau and bureau_balance columns\n",
    "    bureau_days_credit_cols = [ele for ele in bureau_aggregated.columns if 'DAYS_CREDIT' in ele and 'ENDDATE' not in ele and 'UPDATE' not in ele]\n",
    "    for col in bureau_days_credit_cols:\n",
    "        data['BUREAU_' + col + '_EMPLOYED_DIFF'] = data[col] - data['DAYS_EMPLOYED']\n",
    "        data['BUREAU_' + col + '_REGISTRATION_DIFF'] = data[col] - data['DAYS_REGISTRATION']  \n",
    "    bureau_overdue_cols = [ele for ele in bureau_aggregated.columns if 'AMT_CREDIT' in ele and 'OVERDUE' in ele]\n",
    "    for col in bureau_overdue_cols:\n",
    "        data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)\n",
    "    bureau_amt_annuity_cols = [ele for ele in bureau_aggregated.columns if 'AMT_ANNUITY' in ele and 'CREDIT'  not in ele]\n",
    "    for col in bureau_amt_annuity_cols:\n",
    "        data['BUREAU_' + col + '_INCOME_RATIO'] = data[col] / (data['AMT_INCOME_TOTAL'] + 0.00001)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T09:06:45.235385Z",
     "start_time": "2020-10-23T09:06:40.946783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Pre-processing, aggregation, merging and Feature Engineering,\n",
      "Final Shape of Training Data = (246006, 1634)\n",
      "Final Shape of Test Data = (61502, 1633)\n"
     ]
    }
   ],
   "source": [
    "create_new_features(train_data)\n",
    "create_new_features(test_data)\n",
    "\n",
    "print(\"After Pre-processing, aggregation, merging and Feature Engineering,\")\n",
    "print(f\"Final Shape of Training Data = {train_data.shape}\")\n",
    "print(f\"Final Shape of Test Data = {test_data.shape}\")\n",
    "\n",
    "#freeing up the memory\n",
    "del application_train, application_test, bureau_aggregated, previous_aggregated, installments_aggregated, pos_aggregated, cc_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-23T09:07:06.218011Z",
     "start_time": "2020-10-23T09:06:45.236569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumping the final preprocessed data to pickle files.\n",
      "Done.\n",
      "Time elapsed = 0:00:01.702096\n"
     ]
    }
   ],
   "source": [
    "def final_pickle_dump(train_data, test_data, train_file_name, test_file_name, file_directory = '', verbose = True):\n",
    "    '''\n",
    "    Function to dump the preprocessed files to pickle.\n",
    "    \n",
    "    Inputs:\n",
    "        train_data: DataFrame\n",
    "            Training Data\n",
    "        test_data: DataFrame\n",
    "            Test Data\n",
    "        train_file_name: str\n",
    "            Name of pickle file for training data\n",
    "        test_file_name: str\n",
    "            Name of pickle file for test data\n",
    "        file_directory: str, default = ''\n",
    "            Path of directory to save pickle file into\n",
    "        verbose: bool, default = True\n",
    "            Whether to keep verbosity or not\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    if verbose:\n",
    "        print(\"Dumping the final preprocessed data to pickle files.\")\n",
    "        start = datetime.now()\n",
    "    with open(file_directory + train_file_name + '.pkl','wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    with open(file_directory + test_file_name + '.pkl','wb') as f:\n",
    "        pickle.dump(test_data,f)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Done.\")\n",
    "        print(f\"Time elapsed = {datetime.now() - start}\")\n",
    "\n",
    "final_pickle_dump(train_data, test_data, 'train_data_final', 'test_data_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-26T06:12:57.891910Z",
     "start_time": "2020-10-26T06:12:56.211057Z"
    }
   },
   "outputs": [],
   "source": [
    "#removing the SK_ID_CURR from training and test data\n",
    "train_data = train_data.drop(['SK_ID_CURR'], axis = 1)\n",
    "skid_test = test_data.pop('SK_ID_CURR')\n",
    "#extracting the class labels for training data\n",
    "target_train = train_data.pop('TARGET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T12:11:08.459176Z",
     "start_time": "2020-10-20T12:11:02.247305Z"
    }
   },
   "source": [
    "In this section, we will try to reduce the number of features, in such a way that it doesn't have a negative impact on the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for empty features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, empty features refer to those features which have just one unique value. These features are useless for the classifiers, as they do not contain any information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-22T17:16:36.298471Z",
     "start_time": "2020-10-22T17:16:19.243571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23 columns with just 1 unique value\n",
      "Removing these from dataset\n"
     ]
    }
   ],
   "source": [
    "empty_columns = []\n",
    "for col in train_data.columns:\n",
    "    if len(train_data[col].unique()) <=1:\n",
    "        empty_columns.append(col)\n",
    "    \n",
    "print(f\"There are {len(empty_columns)} columns with just 1 unique value\")\n",
    "print(\"Removing these from dataset\")\n",
    "train_data = train_data.drop(empty_columns, axis = 1)\n",
    "test_data = test_data.drop(empty_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Selection using LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will further try to reduce the feature set, using a Classification Model, using the feature importance attribute\n",
    ". \n",
    "In this method, we will recursively run the Classification model on training dataset, and will check the Cross Validation AUC. If the Cross-Validation AUC goes below a certain threshold, we will stop adding the features the features.\n",
    "\n",
    "The steps would be:\n",
    "1. Run the Classifier on whole training set, and calculate 3 fold cross-validation AUC.\n",
    "2. Select the features which have non-zero feature importance as per the model.\n",
    "3. Rerun the Classifier with the features which had zero feature importance. This is done because there might be cases where the classifier would have assigned 0-feature imporatnce to some features but that could be due to just that iteration and randomness. So we rerun the classifier on those features, to see if they alone can give good metric score.\n",
    "4. Stop adding features if the Cross Validation score for low importance features goes below a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Feature Selection...\n",
      "Iteration 1:\n",
      "\t\tFitting fold 1\n",
      "[LightGBM] [Info] Number of positive: 13251, number of negative: 150753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.812920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 276066\n",
      "[LightGBM] [Info] Number of data points in the train set: 164004, number of used features: 1603\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080797 -> initscore=-2.431570\n",
      "[LightGBM] [Info] Start training from score -2.431570\n",
      "\t\tFitting fold 2\n",
      "[LightGBM] [Info] Number of positive: 13251, number of negative: 150753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.802117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 275955\n",
      "[LightGBM] [Info] Number of data points in the train set: 164004, number of used features: 1603\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080797 -> initscore=-2.431570\n",
      "[LightGBM] [Info] Start training from score -2.431570\n",
      "\t\tFitting fold 3\n",
      "[LightGBM] [Info] Number of positive: 13250, number of negative: 150754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.764613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 276016\n",
      "[LightGBM] [Info] Number of data points in the train set: 164004, number of used features: 1593\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080791 -> initscore=-2.431652\n",
      "[LightGBM] [Info] Start training from score -2.431652\n",
      "\tNo. of important columns kept = 1156\n",
      "\tCross Validation score = 0.7937572750705959\n",
      "Iteration 2:\n",
      "\t\tFitting fold 1\n",
      "[LightGBM] [Info] Number of positive: 13251, number of negative: 150753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.125567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 39506\n",
      "[LightGBM] [Info] Number of data points in the train set: 164004, number of used features: 447\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080797 -> initscore=-2.431570\n",
      "[LightGBM] [Info] Start training from score -2.431570\n",
      "\t\tFitting fold 2\n",
      "[LightGBM] [Info] Number of positive: 13251, number of negative: 150753\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.089293 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 39572\n",
      "[LightGBM] [Info] Number of data points in the train set: 164004, number of used features: 447\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080797 -> initscore=-2.431570\n",
      "[LightGBM] [Info] Start training from score -2.431570\n",
      "\t\tFitting fold 3\n",
      "[LightGBM] [Info] Number of positive: 13250, number of negative: 150754\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.356026 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 39489\n",
      "[LightGBM] [Info] Number of data points in the train set: 164004, number of used features: 437\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.080791 -> initscore=-2.431652\n",
      "[LightGBM] [Info] Start training from score -2.431652\n",
      "\tNo. of important columns kept = 1248\n",
      "\tCross Validation score = 0.7110325663716538\n",
      "\n",
      "Done Selecting Features.\n",
      "Total columns removed = 361\n",
      "\n",
      "Initial Shape of train_data = (246006, 1609)\n",
      "Final Shape of train_data = (246006, 1248)\n",
      "\n",
      "Total Time Taken = 0:03:13.858137\n"
     ]
    }
   ],
   "source": [
    "class recursive_feature_selector:\n",
    "    '''\n",
    "    Class to recursively select top features.\n",
    "    Contains 2 methods:\n",
    "        1. init method\n",
    "        2. main method\n",
    "    '''\n",
    "   \n",
    "    def __init__(self, train_data, test_data, target_train, num_folds = 3, verbose = True, random_state = 5358):\n",
    "        '''\n",
    "        Function to initialize the class variables.\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "            train_data: DataFrame\n",
    "                Training Data\n",
    "            test_data: DataFrame\n",
    "                Test Data\n",
    "            target_train: Series\n",
    "                Class Labels for training Data\n",
    "            num_folds: int, default = 3\n",
    "                Number of folds for K-Fold CV\n",
    "            verbose: bool, default = True\n",
    "                Whether to keep verbosity or not\n",
    "            random_state: int, default = 5358\n",
    "                The random state for the classifier for recursive feature selection\n",
    "                \n",
    "        Returns: \n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.target_train = target_train\n",
    "        self.num_folds = num_folds\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Clean column names\n",
    "        self.train_data.columns = self.train_data.columns.str.replace(r'[^A-Za-z0-9_]+', '_', regex=True)\n",
    "        self.test_data.columns = self.test_data.columns.str.replace(r'[^A-Za-z0-9_]+', '_', regex=True)\n",
    "\n",
    "    def main(self):\n",
    "        '''\n",
    "        Function to select features recursively\n",
    "        \n",
    "        Inputs:\n",
    "            self\n",
    "        \n",
    "        Returns:\n",
    "            Training and testing data with reduced number of features\n",
    "        '''\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Starting Feature Selection...\")\n",
    "            start = datetime.now()\n",
    "            \n",
    "        # set of important features\n",
    "        self.important_columns = set()\n",
    "        score = 1\n",
    "        i = 1\n",
    "        \n",
    "        while score > 0.72:\n",
    "            if self.verbose:\n",
    "                print(f\"Iteration {i}:\")\n",
    "                \n",
    "            # removing the features which have been selected from the modelling data\n",
    "            selection_data = self.train_data.drop(list(self.important_columns), axis=1)\n",
    "            # defining the CV strategy\n",
    "            fold = StratifiedKFold(n_splits=self.num_folds, shuffle=True, random_state=33)\n",
    "            # reinitializing the score\n",
    "            score = 0\n",
    "            model_feature_importance = np.zeros_like(selection_data.columns)\n",
    "            \n",
    "            # doing K-Fold Cross validation\n",
    "            for fold_num, (train_indices, val_indices) in enumerate(fold.split(selection_data, self.target_train), 1):\n",
    "                if self.verbose:\n",
    "                    print(f\"\\t\\tFitting fold {fold_num}\")\n",
    "                \n",
    "                # defining the train and validation data\n",
    "                x_train = selection_data.iloc[train_indices]\n",
    "                x_val = selection_data.iloc[val_indices]\n",
    "                y_train = self.target_train.iloc[train_indices]\n",
    "                y_val = self.target_train.iloc[val_indices]\n",
    "                \n",
    "                # instantiating the LightGBM Classifier\n",
    "                lg = LGBMClassifier(n_jobs=-1, random_state=self.random_state)\n",
    "                lg.fit(x_train, y_train)\n",
    "\n",
    "                # appending the feature importance of each feature averaged over different folds\n",
    "                model_feature_importance += lg.feature_importances_ / self.num_folds\n",
    "                # average k-fold ROC-AUC Score\n",
    "                score += roc_auc_score(y_val, lg.predict_proba(x_val)[:, 1]) / self.num_folds\n",
    "\n",
    "            # getting the non-zero feature importance columns\n",
    "            imp_cols_indices = np.where(np.abs(model_feature_importance) > 0)\n",
    "            # names of non-zero feature importance columns\n",
    "            cols_imp = self.train_data.columns[imp_cols_indices]\n",
    "            \n",
    "            if score > 0.7:\n",
    "                self.important_columns.update(cols_imp)\n",
    "                if self.verbose:\n",
    "                    print(f\"\\tNo. of important columns kept = {len(self.important_columns)}\")            \n",
    "            if self.verbose:\n",
    "                print(f\"\\tCross Validation score = {score}\")\n",
    "            i += 1\n",
    "            \n",
    "        self.important_columns = list(self.important_columns)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\nDone Selecting Features.\")\n",
    "            print(f\"Total columns removed = {self.train_data.shape[1] - len(self.important_columns)}\")\n",
    "            print(f\"\\nInitial Shape of train_data = {self.train_data.shape}\")\n",
    "        self.train_data = self.train_data[self.important_columns]\n",
    "        self.test_data = self.test_data[self.important_columns]\n",
    "        if self.verbose:\n",
    "            print(f\"Final Shape of train_data = {self.train_data.shape}\")\n",
    "            print(f\"\\nTotal Time Taken = {datetime.now() - start}\")\n",
    "            \n",
    "        # saving the final columns into a pickle file\n",
    "        with open('final_cols.pkl', 'wb') as f:\n",
    "            pickle.dump(self.important_columns, f)\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "        return self.train_data, self.test_data\n",
    "\n",
    "\n",
    "# Instantiate the class recursive_feature_selector\n",
    "feature_selector = recursive_feature_selector(train_data, test_data, target_train)\n",
    "train_data, test_data = feature_selector.main()\n",
    "important_columns = feature_selector.important_columns\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
